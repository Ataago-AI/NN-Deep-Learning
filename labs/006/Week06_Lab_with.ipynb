{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yESTaUtabNpA"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsoK5jBsbNpF"},"outputs":[],"source":["import my_utils as mu\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"S2izhX3CbNpF"},"source":["# The Task\n","\n","* Our goal for this week is to write some code to create an MLP and then experiment with several training options and hyper-parameters.\n","\n","* For this reason we will use again the same pipeline we used last week! \n","    \n","* The Learning Outcome: Hands-on application of PyTorch's API for creating and training MLPs for classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU244TLybNpG"},"outputs":[],"source":["# Read training and test data\n","batch_size = 256\n","train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)\n","# type(train_iter)"]},{"cell_type":"markdown","metadata":{"id":"SAmdYIP7bNpG","origin_pos":4},"source":["## Task 1\n","\n","\n","* The model below implements Softmax Regression. It has 1 Linear (or Fully-Connected Layer). Expand the `Net` class to have one more linear layer as follows: the 1st Linear layer will output `num_hidden` outputs. Set `num_hidden=256`.  Moreover, apply a `ReLU` activation function to the output of the 1st Linear layer. The second linear layer will have `num_hidden` inputs and 10 outputs (i.e. equal to the number of classes).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIweuroNbNpH"},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self, num_inputs, num_hdden, num_outputs):\n","        super(Net, self).__init__()\n","        self.num_inputs = num_inputs\n","        self.num_hidden = num_hidden\n","        self.num_outputs = num_outputs\n","        self.Linear1 = nn.Linear(num_inputs, num_hidden)\n","        self.Linear2 = nn.Linear(num_hidden, num_outputs)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, self.num_inputs)\n","        x = self.Linear1(x)\n","        x = self.relu(x)\n","        out = self.Linear2(x)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhlBwFegbNpH","origin_pos":6,"tab":["pytorch"]},"outputs":[],"source":["# Model instantiation and initialisation \n","def init_weights(m):\n","    if type(m) == nn.Linear: # by checking type we can init different layers in different ways\n","        torch.nn.init.normal_(m.weight, std=0.01)\n","        torch.nn.init.zeros_(m.bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSD17MRPbNpH"},"outputs":[],"source":["# Create and initialize your model here:\n","num_inputs, num_hidden, num_outputs = 784, 256, 10\n","net = Net(num_inputs,num_hidden, num_outputs)"]},{"cell_type":"markdown","metadata":{"id":"vJHLwt8vbNpH","origin_pos":8},"source":["# Loss and Optimization Algorithm\n","* As in Softmax Regression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNgzCdWObNpI","origin_pos":10,"tab":["pytorch"]},"outputs":[],"source":["# Creare your loss here. Use Cross Entropy loss:\n","loss = nn.CrossEntropyLoss()\n","lr, wd = 0.1, 0\n","# Creare your optimizer here. Use SGD with weight decay wd and learning rate lr.\n","optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)"]},{"cell_type":"markdown","metadata":{"id":"hiBOrs2pbNpI"},"source":["# Training\n","\n","* Use `my_utils.train_ch3` as in Softmax Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik7KovzgbNpI","origin_pos":12,"tab":["pytorch"]},"outputs":[],"source":["num_epochs = 60 # learning rate 0.1\n","mu.train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"Qaak5FgZbNpJ"},"source":["## Further Tasks\n","\n","* Explore the following training options and observe their impact on the evolution of the **training loss**, the **training accuracy** and the **validation accuracy**. Different choices might have different impact on both accuracy and convergence:\n","    1. Change the number of hidden layers to 256 \n","    1. Investigate different learning rates. Use 0.5, 0.9 and 0.01.\n","    1. Investigate adding weight decay wd=0.0005\n","    1. Investigate `Sigmoid` activation function. \n","    1. Try different schemes for initializing the weights. "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
