{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SYeUl0GqTiPg"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKJRukyjTeTR"},"outputs":[],"source":["import my_utils as mu\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"UPMrBI7gTfSC"},"source":["# The Task\n","\n","* Our **Task** for this week is to implement LeNet.\n","* The Learning Outcome: Hands-on application of PyTorch's API for creating and training CNNs.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gh0thI17TeTU"},"source":["# LeNet\n","\n","* At a high level, LeNet (LeNet-5) consists of 2 parts:\n","    1. a convolutional encoder consisting of two convolutional layers; and\n","    2. a dense block consisting of three fully-connected layers;\n","\n","\n","<!-- ![Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.](https://drive.google.com/uc?export=view&id=18Kd-JNGeKp38qAVEuxEyYU7rjNudWdWA) -->\n","\n","\n","![Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.](https://drive.google.com/uc?export=view&id=13EvZlXCSIovK6jA8uRvyZp9EE-PquOUx) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"uUTBAukyTeTV"},"source":["# LeNet -- Convolutional Encoder\n","\n","* Each convolutional *block* consists of: \n","    * A convolutional layer.\n","    * A sigmoid activation function (ReLUs were discovered recently).\n","    * A subsequent average pooling operation (max pooling was discovered later).\n","* Each convolutional layer uses a $5\\times 5$ kernel.\n","* The first convolutional layer has 6 output channels, while the second has 16.\n","* Each $2\\times2$ pooling operation (stride 2) reduces dimensionality by a factor of $4$ via spatial downsampling.\n","* The convolutional block emits an output with shape given by (batch size, number of channels, height, width).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U1e7kHElTeTV"},"source":["# LeNet -- Dense Block\n","\n","* In order to pass the output from the convolutional block to the dense block, we must flatten each example in the minibatch.\n","* In other words, we take the four-dimensional input and transform it into the two-dimensional input expected by fully-connected layers:\n","    * the two-dimensional representation that we desire has uses the first dimension to index examples in the minibatch\n","    * the second to give the flat vector representation of each example.\n","* LeNet's dense block has three fully-connected layers, with 120, 84, and 10 outputs, respectively.\n","    * Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes."]},{"cell_type":"markdown","metadata":{"id":"6fWxaKgJTeTW"},"source":["# Compressed LeNet Representation \n","\n","\n","\n","![Compressed notation for LeNet-5.](https://drive.google.com/uc?export=view&id=1Oh-SnOYVTCH0WZGbsGqzo1Mju6TYC8ue)\n"]},{"cell_type":"markdown","metadata":{"id":"ZuGxje7cTeTW"},"source":["# Concise Implementation of LeNet\n","\n","* Goal: use high-level APIs of PyTorch for implementing LeNet for classification. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9fJJAC9TeTW"},"outputs":[],"source":["# Read training and test data\n","batch_size = 256\n","train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)\n","# type(train_iter)"]},{"cell_type":"markdown","metadata":{"id":"EknCtScjTeTX"},"source":["# Defining the Model\n","\n","* We will modify the code from MLP\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb5aJZHBQ7TZ"},"outputs":[],"source":["class LeNet(torch.nn.Module):\n","    def __init__(self, num_inputs, num_outputs):\n","        super(LeNet, self).__init__()\n","        self.num_inputs = num_inputs\n","        self.num_outputs = num_outputs\n","        C1 = 6\n","        C2 = 16\n","        L1 = 400\n","        L2 = 120\n","        L3 = 84\n","        self.Convl1 = nn.Conv2d(num_inputs, C1, kernel_size = 5, padding = 2)\n","        self.Sigmoid = nn.Sigmoid()\n","        self.Avg1 = nn.AvgPool2d(2, stride=2, padding=0)\n","        self.Convl2 = nn.Conv2d(C1,C2,kernel_size = 5)\n","        self.Avg2 = nn.AvgPool2d(2, stride=2, padding=0)\n","        self.Fltn = nn.Flatten()\n","        self.Linear1 = nn.Linear(L1, L2)\n","        self.Linear2 = nn.Linear(L2, L3)\n","        self.Linear3 = nn.Linear(L3, num_outputs)\n","    def forward(self, x):\n","        out = self.Convl1(x)\n","        out = self.Sigmoid(out)\n","        out = self.Avg1(out)\n","        out = self.Convl2(out)\n","        out = self.Sigmoid(out)\n","        out = self.Avg2(out)\n","        #out = self.Sigmoid(out)\n","        out = self.Fltn(out)\n","        out = self.Linear1(out)\n","        out = self.Sigmoid(out)\n","        out = self.Linear2(out)\n","        out = self.Sigmoid(out)\n","        out = self.Linear3(out)\n","        # out = self.Sigmoid(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLQCSd34TeTX"},"outputs":[],"source":["def init_weights(m):\n","    if type(m) == nn.Linear or type(m) == nn.Conv2d: # by checking the type we can init different layers in different ways\n","        torch.nn.init.xavier_uniform_(m.weight)          \n","\n","num_outputs = 10\n","#model = LeNet(num_outputs)\n","model = LeNet(1, num_outputs)\n","model.apply(init_weights);\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"3vwPW1iETeTY"},"source":["# Loss and Optimization Algorithm\n","* As in Softmax Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRRadXXxTeTY"},"outputs":[],"source":["loss = nn.CrossEntropyLoss()\n","lr = 0.9\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)"]},{"cell_type":"markdown","metadata":{"id":"ULuAOHDXTeTY"},"source":["# Training\n","\n","* Use `my_utils.train_ch3` as in Softmax Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJr4qkTJTeTY","scrolled":false},"outputs":[],"source":["num_epochs = 20\n","mu.train_ch3(model, train_iter, test_iter, loss, num_epochs, optimizer)"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}