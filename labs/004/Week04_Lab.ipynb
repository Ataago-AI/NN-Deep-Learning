{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHj-TUOEcErV",
        "outputId": "cf140e11-7fbf-4305-a2b2-8aa00711a8e5"
      },
      "outputs": [],
      "source": [
        "#Uncomment if you're using colab\n",
        "#Setting up google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCrE84owcErW",
        "outputId": "693ac7f5-278b-4d52-f07e-8a185d72cd32"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import my_utils as mu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06W5TXm5cErX"
      },
      "source": [
        "# The Task\n",
        "\n",
        "* Our **Task** for this week is to apply Linear Regression to Sequential Data; in particular to time series. The model that we will develop is a simple *Auto-Regressive* Model which is an widely-used family of models in Machine Learning. \n",
        "* The Learning Outcome: Hands-on application of PyTorch's API for solving Linear Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMOnsaevcErX"
      },
      "source": [
        "# Data Generation\n",
        "\n",
        "* We generate our sequence data by using a function $x = f(t)$ with some additive noise for time steps $1, 2, \\ldots, 1000$.\n",
        "* We define the function $f(.)$ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dcA3vzKccErX",
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "def f(time):\n",
        "    return torch.sin(0.1 * time) * torch.exp(-0.001*time)# + torch.normal(0, 0.2, (T,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ovxkmzZPcErY",
        "outputId": "3309180b-a469-41f8-9294-3f90b86fdb2d"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"406.885938pt\" height=\"207.83625pt\" viewBox=\"0 0 406.885938 207.83625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-02-24T16:23:38.037969</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 207.83625 \nL 406.885938 207.83625 \nL 406.885938 0 \nL 0 0 \nL 0 207.83625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.160938 170.28 \nL 386.960938 170.28 \nL 386.960938 7.2 \nL 52.160938 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 118.852829 170.28 \nL 118.852829 7.2 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m1a6c425887\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1a6c425887\" x=\"118.852829\" y=\"170.28\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 200 -->\n      <g transform=\"translate(109.309079 184.878438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 185.879856 170.28 \nL 185.879856 7.2 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m1a6c425887\" x=\"185.879856\" y=\"170.28\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 400 -->\n      <g transform=\"translate(176.336106 184.878438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 252.906883 170.28 \nL 252.906883 7.2 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m1a6c425887\" x=\"252.906883\" y=\"170.28\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 600 -->\n      <g transform=\"translate(243.363133 184.878438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 319.93391 170.28 \nL 319.93391 7.2 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m1a6c425887\" x=\"319.93391\" y=\"170.28\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 800 -->\n      <g transform=\"translate(310.39016 184.878438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 386.960938 170.28 \nL 386.960938 7.2 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m1a6c425887\" x=\"386.960938\" y=\"170.28\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1000 -->\n      <g transform=\"translate(374.235937 184.878438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- time -->\n     <g transform=\"translate(208.264844 198.556563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"66.992188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"164.404297\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 52.160938 166.385634 \nL 386.960938 166.385634 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"mf2205c6f96\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf2205c6f96\" x=\"52.160938\" y=\"166.385634\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −1.0 -->\n      <g transform=\"translate(20.878125 170.184853)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 52.160938 128.130778 \nL 386.960938 128.130778 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mf2205c6f96\" x=\"52.160938\" y=\"128.130778\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.5 -->\n      <g transform=\"translate(20.878125 131.929997)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 52.160938 89.875922 \nL 386.960938 89.875922 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mf2205c6f96\" x=\"52.160938\" y=\"89.875922\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(29.257812 93.675141)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 52.160938 51.621066 \nL 386.960938 51.621066 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mf2205c6f96\" x=\"52.160938\" y=\"51.621066\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <g transform=\"translate(29.257812 55.420285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 52.160938 13.36621 \nL 386.960938 13.36621 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#mf2205c6f96\" x=\"52.160938\" y=\"13.36621\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(29.257812 17.165429)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- x -->\n     <g transform=\"translate(14.798438 91.699375)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 52.160938 82.24533 \nL 53.836613 46.933718 \nL 54.842019 30.480775 \nL 55.512289 22.435835 \nL 56.182559 17.106537 \nL 56.517694 15.527644 \nL 56.852829 14.694093 \nL 57.187965 14.612727 \nL 57.5231 15.282855 \nL 57.858235 16.69628 \nL 58.528505 21.683421 \nL 59.198775 29.364109 \nL 59.869046 39.421981 \nL 60.874451 58.048267 \nL 62.885262 101.553217 \nL 64.225802 128.941054 \nL 65.231208 145.508265 \nL 65.901478 153.817212 \nL 66.571748 159.548681 \nL 66.906883 161.375496 \nL 67.242019 162.484907 \nL 67.577154 162.867273 \nL 67.912289 162.520218 \nL 68.247424 161.448676 \nL 68.582559 159.664796 \nL 69.252829 154.043765 \nL 69.9231 145.891885 \nL 70.928505 129.674292 \nL 72.269046 102.988372 \nL 74.614992 54.556688 \nL 75.620397 37.925306 \nL 76.290667 29.389893 \nL 76.960938 23.295385 \nL 77.631208 19.874509 \nL 77.966343 19.210246 \nL 78.301478 19.25274 \nL 78.636613 20.000156 \nL 78.971748 21.443634 \nL 79.642019 26.348775 \nL 80.312289 33.76215 \nL 81.317694 48.893941 \nL 82.658235 74.321559 \nL 85.339316 127.53788 \nL 86.344721 142.862737 \nL 87.014992 150.472508 \nL 87.685262 155.640689 \nL 88.020397 157.243906 \nL 88.355532 158.171401 \nL 88.690667 158.415265 \nL 89.025802 157.974435 \nL 89.360938 156.854686 \nL 89.696073 155.068536 \nL 90.366343 149.580155 \nL 91.036613 141.737986 \nL 92.042019 126.286705 \nL 93.382559 101.068596 \nL 95.728505 55.73674 \nL 96.73391 40.333915 \nL 97.404181 32.500278 \nL 98.074451 26.980951 \nL 98.409586 25.158787 \nL 98.744721 23.986316 \nL 99.079856 23.47394 \nL 99.414992 23.625462 \nL 99.750127 24.43804 \nL 100.085262 25.90224 \nL 100.755532 30.715514 \nL 101.425802 37.863628 \nL 102.431208 52.298787 \nL 103.771748 76.347332 \nL 106.452829 126.150483 \nL 107.458235 140.318805 \nL 108.128505 147.281824 \nL 108.798775 151.932736 \nL 109.13391 153.332412 \nL 109.469046 154.095812 \nL 109.804181 154.216597 \nL 110.139316 153.694863 \nL 110.474451 152.537081 \nL 110.809586 150.756074 \nL 111.479856 145.406505 \nL 112.150127 137.868724 \nL 113.155532 123.154676 \nL 114.496073 99.331428 \nL 116.842019 56.913475 \nL 117.847424 42.655244 \nL 118.517694 35.471591 \nL 119.187965 30.481368 \nL 119.5231 28.871812 \nL 119.858235 27.874462 \nL 120.19337 27.498056 \nL 120.528505 27.745085 \nL 120.86364 28.611863 \nL 121.198775 30.08849 \nL 121.869046 34.801514 \nL 122.539316 41.687249 \nL 123.544721 55.450559 \nL 124.885262 78.187364 \nL 127.231208 119.639482 \nL 128.236613 133.944931 \nL 128.906883 141.317267 \nL 129.577154 146.613325 \nL 130.247424 149.630561 \nL 130.582559 150.24626 \nL 130.917694 150.258067 \nL 131.252829 149.667066 \nL 131.587965 148.480367 \nL 132.258235 144.377858 \nL 132.928505 138.122986 \nL 133.93391 125.289072 \nL 135.274451 103.632908 \nL 137.955532 58.082433 \nL 138.960938 44.890359 \nL 139.631208 38.308347 \nL 140.301478 33.804584 \nL 140.636613 32.388896 \nL 140.971748 31.549913 \nL 141.306883 31.294863 \nL 141.642019 31.625122 \nL 141.977154 32.536204 \nL 142.312289 34.01788 \nL 142.982559 38.623776 \nL 143.652829 45.250612 \nL 144.658235 58.366767 \nL 145.998775 79.856134 \nL 148.344721 118.660642 \nL 149.350127 131.912189 \nL 150.020397 138.681007 \nL 150.690667 143.480736 \nL 151.025802 145.081696 \nL 151.360938 146.128352 \nL 151.696073 146.611359 \nL 152.031208 146.52703 \nL 152.366343 145.877346 \nL 152.701478 144.669907 \nL 153.371748 140.639866 \nL 154.042019 134.605997 \nL 155.047424 122.359721 \nL 156.387965 101.874815 \nL 159.069046 59.24006 \nL 160.074451 47.040521 \nL 160.744721 41.015247 \nL 161.414992 36.958269 \nL 161.750127 35.719176 \nL 162.085262 35.023155 \nL 162.420397 34.876121 \nL 162.755532 35.278443 \nL 163.090667 36.224958 \nL 163.425802 37.705156 \nL 164.096073 42.198162 \nL 164.766343 48.570394 \nL 165.771748 61.063484 \nL 167.112289 81.367459 \nL 169.458235 117.682733 \nL 170.46364 129.952299 \nL 171.13391 136.161937 \nL 171.804181 140.50494 \nL 172.139316 141.921735 \nL 172.474451 142.816099 \nL 172.809586 143.180214 \nL 173.144721 143.011464 \nL 173.479856 142.312642 \nL 173.814992 141.091709 \nL 174.485262 137.141666 \nL 175.155532 131.326743 \nL 176.160938 119.647444 \nL 177.501478 100.277034 \nL 179.847424 64.800795 \nL 180.852829 52.497389 \nL 181.5231 46.130624 \nL 182.19337 41.529963 \nL 182.86364 38.87143 \nL 183.198775 38.30427 \nL 183.53391 38.253048 \nL 183.869046 38.717277 \nL 184.204181 39.691255 \nL 184.874451 43.120635 \nL 185.544721 48.396791 \nL 186.550127 59.280471 \nL 187.890667 77.723264 \nL 190.571748 116.709469 \nL 191.577154 128.064321 \nL 192.247424 133.756206 \nL 192.917694 137.679311 \nL 193.252829 138.928008 \nL 193.587965 139.684563 \nL 193.9231 139.942395 \nL 194.258235 139.699908 \nL 194.59337 138.96051 \nL 194.928505 137.732621 \nL 195.598775 133.868901 \nL 196.269046 128.270056 \nL 197.274451 117.13745 \nL 198.614992 98.826873 \nL 200.960938 65.612051 \nL 201.966343 54.212474 \nL 202.636613 48.36481 \nL 203.306883 44.192277 \nL 203.977154 41.854221 \nL 204.312289 41.402894 \nL 204.647424 41.436353 \nL 204.982559 41.953299 \nL 205.317694 42.947589 \nL 205.987965 46.320147 \nL 206.658235 51.412394 \nL 207.66364 61.80035 \nL 209.004181 79.248462 \nL 211.685262 115.74427 \nL 212.690667 126.247353 \nL 213.360938 131.46005 \nL 214.031208 134.997237 \nL 214.366343 136.092816 \nL 214.701478 136.724865 \nL 215.036613 136.887952 \nL 215.371748 136.581424 \nL 215.706883 135.809237 \nL 216.042019 134.58009 \nL 216.712289 130.8079 \nL 217.382559 125.422029 \nL 218.387965 114.815736 \nL 219.728505 97.51247 \nL 222.074451 66.424162 \nL 223.079856 55.866824 \nL 223.750127 50.500273 \nL 224.420397 46.72184 \nL 224.755532 45.475865 \nL 225.090667 44.675659 \nL 225.425802 44.328257 \nL 225.760938 44.436228 \nL 226.096073 44.997596 \nL 226.431208 46.005872 \nL 227.101478 49.314823 \nL 227.771748 54.224595 \nL 228.777154 64.134024 \nL 230.117694 80.635538 \nL 232.798775 114.790036 \nL 233.804181 124.500246 \nL 234.474451 129.269663 \nL 235.144721 132.452502 \nL 235.479856 133.408704 \nL 235.814992 133.92845 \nL 236.150127 134.007375 \nL 236.485262 133.645591 \nL 236.820397 132.8476 \nL 237.490667 129.982557 \nL 238.160938 125.533036 \nL 238.831208 119.682559 \nL 239.836613 108.813757 \nL 241.847424 83.333383 \nL 243.187965 67.233642 \nL 244.19337 57.46124 \nL 244.86364 52.540083 \nL 245.53391 49.124265 \nL 245.869046 48.023906 \nL 246.204181 47.343582 \nL 246.539316 47.089179 \nL 246.874451 47.262426 \nL 247.209586 47.860706 \nL 247.544721 48.877245 \nL 248.214992 52.117021 \nL 248.885262 56.846242 \nL 249.890667 66.294359 \nL 251.231208 81.895306 \nL 253.577154 110.323747 \nL 254.582559 120.129132 \nL 255.252829 125.180101 \nL 255.9231 128.806105 \nL 256.59337 130.868543 \nL 256.928505 131.287173 \nL 257.26364 131.291587 \nL 257.598775 130.882548 \nL 257.93391 130.064985 \nL 258.604181 127.244094 \nL 259.274451 122.947534 \nL 260.279856 114.136916 \nL 261.620397 99.276593 \nL 264.301478 68.037841 \nL 265.306883 58.996346 \nL 265.977154 54.487585 \nL 266.647424 51.40502 \nL 266.982559 50.437399 \nL 267.317694 49.865488 \nL 267.652829 49.694124 \nL 267.987965 49.924211 \nL 268.3231 50.552644 \nL 268.658235 51.572458 \nL 269.328505 54.738198 \nL 269.998775 59.289423 \nL 271.004181 68.292987 \nL 272.344721 83.037954 \nL 274.690667 109.650006 \nL 275.696073 118.732956 \nL 276.366343 123.370106 \nL 277.036613 126.656112 \nL 277.371748 127.750873 \nL 277.706883 128.465365 \nL 278.042019 128.793212 \nL 278.377154 128.731922 \nL 278.712289 128.282824 \nL 279.047424 127.451261 \nL 279.717694 124.680607 \nL 280.387965 120.53595 \nL 281.39337 112.129149 \nL 282.73391 98.072798 \nL 285.414992 68.83392 \nL 286.420397 60.473033 \nL 287.090667 56.34578 \nL 287.760938 53.569355 \nL 288.096073 52.722715 \nL 288.431208 52.248646 \nL 288.766343 52.151139 \nL 289.101478 52.430434 \nL 289.436613 53.082966 \nL 290.106883 55.475062 \nL 290.777154 59.226584 \nL 291.447424 64.18255 \nL 292.452829 73.422474 \nL 294.46364 95.175708 \nL 295.804181 108.977179 \nL 296.809586 117.38664 \nL 297.479856 121.640704 \nL 298.150127 124.613682 \nL 298.485262 125.582311 \nL 298.820397 126.192609 \nL 299.155532 126.439095 \nL 299.490667 126.320102 \nL 299.825802 125.837553 \nL 300.160938 124.996976 \nL 300.831208 122.281543 \nL 301.501478 118.287602 \nL 302.506883 110.270097 \nL 303.847424 96.978646 \nL 306.19337 72.64857 \nL 307.198775 64.215207 \nL 307.869046 59.853238 \nL 308.539316 56.703287 \nL 309.209586 54.8858 \nL 309.544721 54.499909 \nL 309.879856 54.467925 \nL 310.214992 54.789461 \nL 310.550127 55.46066 \nL 311.220397 57.818811 \nL 311.890667 61.443096 \nL 312.896073 68.915037 \nL 314.236613 81.57041 \nL 316.917694 108.3075 \nL 317.9231 116.090023 \nL 318.59337 119.989056 \nL 319.26364 122.674405 \nL 319.598775 123.527946 \nL 319.93391 124.043857 \nL 320.269046 124.217656 \nL 320.604181 124.048274 \nL 320.939316 123.538107 \nL 321.274451 122.692929 \nL 321.944721 120.037207 \nL 322.614992 116.191803 \nL 323.620397 108.549898 \nL 324.960938 95.985915 \nL 327.306883 73.206862 \nL 328.312289 65.393419 \nL 328.982559 61.387102 \nL 329.652829 58.530522 \nL 330.3231 56.932579 \nL 330.658235 56.625928 \nL 330.99337 56.651842 \nL 331.328505 57.009359 \nL 331.66364 57.69425 \nL 332.33391 60.013112 \nL 333.004181 63.510897 \nL 334.009586 70.642327 \nL 335.350127 82.614714 \nL 338.031208 107.643709 \nL 339.036613 114.84222 \nL 339.706883 118.412809 \nL 340.377154 120.833662 \nL 340.712289 121.582398 \nL 341.047424 122.013065 \nL 341.382559 122.122066 \nL 341.717694 121.908952 \nL 342.052829 121.376502 \nL 342.387965 120.530589 \nL 343.058235 117.938007 \nL 343.728505 114.239182 \nL 344.73391 106.958574 \nL 346.074451 95.08607 \nL 348.420397 73.765836 \nL 349.425802 66.529752 \nL 350.096073 62.853272 \nL 350.766343 60.266618 \nL 351.101478 59.414705 \nL 351.436613 58.868568 \nL 351.771748 58.633052 \nL 352.106883 58.709878 \nL 352.442019 59.097707 \nL 352.777154 59.791983 \nL 353.447424 62.06679 \nL 354.117694 65.439189 \nL 355.1231 72.241744 \nL 356.46364 83.564615 \nL 359.144721 106.987547 \nL 360.150127 113.642293 \nL 360.820397 116.909205 \nL 361.490667 119.087275 \nL 361.825802 119.740516 \nL 362.160938 120.094334 \nL 362.496073 120.145786 \nL 362.831208 119.894954 \nL 363.166343 119.344962 \nL 363.836613 117.37476 \nL 364.506883 114.318112 \nL 365.177154 110.301205 \nL 366.182559 102.841697 \nL 368.19337 85.362133 \nL 369.53391 74.322774 \nL 370.539316 67.624939 \nL 371.209586 64.253664 \nL 371.879856 61.915501 \nL 372.214992 61.163296 \nL 372.550127 60.699191 \nL 372.885262 60.527315 \nL 373.220397 60.648747 \nL 373.555532 61.061697 \nL 373.890667 61.761453 \nL 374.560938 63.988496 \nL 375.231208 67.236647 \nL 376.236613 73.722418 \nL 377.577154 84.427266 \nL 379.9231 103.923713 \nL 380.928505 110.644722 \nL 381.598775 114.105147 \nL 382.269046 116.587672 \nL 382.939316 117.997465 \nL 383.274451 118.282098 \nL 383.609586 118.282598 \nL 383.944721 117.999531 \nL 384.279856 117.436241 \nL 384.950127 115.496646 \nL 385.620397 112.545346 \nL 386.625802 106.49671 \nL 386.960938 104.128268 \nL 386.960938 104.128268 \n\" clip-path=\"url(#pc91dfac200)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 52.160938 170.28 \nL 52.160938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.960938 170.28 \nL 386.960938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 52.160938 170.28 \nL 386.960938 170.28 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 52.160938 7.2 \nL 386.960938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc91dfac200\">\n   <rect x=\"52.160938\" y=\"7.2\" width=\"334.8\" height=\"163.08\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 432x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run the following code\n",
        "T = 1000  # Generate a total of 1000 points\n",
        "time = torch.arange(1, T + 1, dtype=torch.float32)\n",
        "x = f(time)\n",
        "# Uncomment line below for Task 6\n",
        "#x = 1.0/ (1.0 + f(time))\n",
        "mu.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1_JaMwRcErY"
      },
      "source": [
        "## Question\n",
        "* Do you expect that a linear model can approximately well the above function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMcW0XpTcErY",
        "origin_pos": 6
      },
      "source": [
        "## Task 1\n",
        "* Your first task is to turn the above sequence into features and labels that we can train our model on.\n",
        "* You should create your dataset as follows: for every time instance $t$, the target (i.e. ground truth label) will be $y_t = x_t$ and the input features $\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}]$. \n",
        "    * Choose $\\tau=4$ and start from $t=\\tau+1$. So the first (input features, target) pair should be $([x_0, x_1, x_2, x_3], x_4)$, the second $([x_1, x_2, x_3, x_4], x_5)$ and so on.\n",
        "* Store the features in a matrix (using a tensor from `torch`) called `features` so that the first row contains the first input feature, the second row the second input feature and so on. For example `features[0, :]= x_0, x_1, x_2, x_3`.\n",
        "*  Store the labels in a vector (using a tensor from `torch`) called `labels` so that the first element contains the first target, the second row the second label and so on.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YiO-m-FhcErZ",
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "# Write your code here to create the two tensors: features and labels\n",
        "tau = 4\n",
        "features = torch.zeros((T - tau, tau))\n",
        "for i in range(tau):\n",
        "    features[:, i] = x[i: T - tau + i]\n",
        "labels = mu.reshape(x[tau:], (-1, 1)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG19XC60cErZ"
      },
      "outputs": [],
      "source": [
        "# Might be good idea to check here that the features and labels tensors contain the right data \n",
        "# by printing the values they contain. What are their dimensions?\n",
        "print(features.shape)\n",
        "print(labels.shape)\n",
        "print(x[:7])\n",
        "print(features[0:3])\n",
        "print(labels[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZAOcjmDcErZ"
      },
      "source": [
        "# Task 2\n",
        "* Use `data.TensorDataset` and the previously created matrices to create your dataset. Simply call it `dataset`. \n",
        "    * Use the first 600 feature-label pairs for training.\n",
        "    * Check lecture slides and `torch` documentation if necessary.\n",
        "* Use `data.DataLoader` to create your dataloader. Pick a reasonable value for the batch size (e.g. `batch_size=16`).\n",
        "    * You can call your dataloader `data_iter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i88ixzYbcErZ",
        "outputId": "a16b2b51-da9f-4f7f-ae40-20d4ce51236b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.0997, 0.1983, 0.2946, 0.3879]), tensor([0.4770]))\n"
          ]
        }
      ],
      "source": [
        "# Write your code here to create dataset and data_iter\n",
        "n_train = 600 # number of feature-labels pairs used for training\n",
        "dataset = data.TensorDataset(features[:n_train], labels[:n_train]) #TensorDataset object\n",
        "print(dataset[0]) #First example in our dataset\n",
        "batch_size = 16\n",
        "data_iter =  data.DataLoader(dataset, batch_size, shuffle=True) #DataLoader object\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUoVBQ4cEra"
      },
      "source": [
        "# Task 3\n",
        "* This task is about creating, initializing and training a linear net, using PyTorch's API. You have to implement the following 4 steps: \n",
        "    1. Create a simple linear net and initialize it appropriately.\n",
        "    1. Use an MSE loss.\n",
        "    1. From `torch.optim` use Adam to create an `optimizer`.\n",
        "    1. Write a training function with the following signature `train(net, data_iter, loss, optimizer, epochs)` and use it to train your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZCfhmI7XcEra"
      },
      "outputs": [],
      "source": [
        "# Write code for defining and initializing your model here\n",
        "num_of_inp, num_of_out = 4, 1 # input, output feature dimension\n",
        "\n",
        "net = nn.Linear(num_of_inp, num_of_out)\n",
        "net.weight.data.normal_(0, 0.01); # each weight sampled from a Gaussian with mean 0 and std 0.01.\n",
        "net.bias.data.fill_(0);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AiyS98a1cEra"
      },
      "outputs": [],
      "source": [
        "# Write code for creating the loss and the optimizer here\n",
        "loss = nn.MSELoss()\n",
        "lr = 0.01\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N8xuz4J6cEra"
      },
      "outputs": [],
      "source": [
        "# Write code for the training function train(net, data_iter, loss, optimizer, epochs) here\n",
        "def train(net, train_iter, loss, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "            l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch + 1}, 'f'loss: {mu.evaluate_loss(net, train_iter, loss):f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-vFAmAcErb"
      },
      "outputs": [],
      "source": [
        "# Write code for training the model here. This refers to calling the train function.\n",
        "train(net, data_iter, loss, optimizer, 400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGbR-dgucErb",
        "origin_pos": 18
      },
      "source": [
        "## Task 4 \n",
        "\n",
        "* Use the model to *predict* what happens in the next time step. \n",
        "    * This is called *one-step-ahead prediction*.\n",
        "    * Store the ouput of your model in a variable call `onestep_preds` and use the following code to plot the original data and the model's prediction.\n",
        "    * What do you oberve? Can the model predict the output samples correctly? Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C7vfHtSMcErb"
      },
      "outputs": [],
      "source": [
        "# Write code for getting the model's predictions onestep_preds here\n",
        "onestep_preds = net(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSuBf0I8cErb"
      },
      "outputs": [],
      "source": [
        "# this will not work if the you don't provide onestep_preds :)\n",
        "mu.plot([time, time[tau:]], [mu.numpy(x), mu.numpy(onestep_preds)], 'time',\n",
        "         'x', legend=['data', '1-step preds'], xlim=[1, 1000],\n",
        "         figsize=(6, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuTl8Ju0cErb"
      },
      "source": [
        "## Task 5\n",
        "* Modify the above code to plot the absolute difference between tha data and the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFpJ43X-cErb"
      },
      "outputs": [],
      "source": [
        "# Write the modified plot function here\n",
        "mu.plot(time[tau:], mu.numpy((x[tau:]-onestep_preds.squeeze(1)).abs()) , 'time',\n",
        "        'x', legend=['error'], xlim=[1, 1000],\n",
        "         figsize=(6, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-_VkvvcErc"
      },
      "source": [
        "## Task 6\n",
        "\n",
        "* Change the generating function to $x = \\frac{1.0}{1.0 + f(t)}$ and repeat the above mentioned steps.\n",
        "* What do you observe? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kSK8of4cErc"
      },
      "source": [
        "## Task 7 -- Optional\n",
        "\n",
        "* Re-run your code with the original generating function to $x = f(t)$.\n",
        "* It turns out that our previous predictions were too good to be true. The reason for this is that after we train the model using the first let's say 600 samples we cannot expect to have the inputs for all one-step-ahead predictions beyond this point.\n",
        "\n",
        "* Instead, we need to work our way forward one step at a time:\n",
        "\n",
        "$$\n",
        "\\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\\\\n",
        "\\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \\hat{x}_{605}), \\\\\n",
        "\\hat{x}_{607} = f(x_{603}, x_{604}, \\hat{x}_{605}, \\hat{x}_{606}),\\\\\n",
        "\\hat{x}_{608} = f(x_{604}, \\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}),\\\\\n",
        "\\hat{x}_{609} = f(\\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}, \\hat{x}_{608}),\\\\\n",
        "\\ldots\n",
        "$$\n",
        "\n",
        "* In other words, we will have to use our own predictions to make multistep-ahead predictions.\n",
        "* You are tasked to write some code to be able to do this. Store the model's prediction in variable called `multistep_preds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tQTaSTXsanyg"
      },
      "outputs": [],
      "source": [
        "# Write the code to obtain the multistep-ahead predictions (multistep_preds) \n",
        "multistep_preds = torch.zeros(T)\n",
        "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
        "for i in range(n_train + tau, T):\n",
        "    multistep_preds[i] = mu.reshape(net(multistep_preds[i - tau: i].reshape(1, -1)), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LYOL42EcErd"
      },
      "source": [
        "## Task 8 -- Optional\n",
        "* Modify the plotting code above to plot the original data, the one step predictions from Task 4 (`onestep_preds`) along with the multistep-ahead predictions from Task 7.\n",
        "* What do you observe? Do you have some explanation for what happens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAE1B9yNanyi"
      },
      "outputs": [],
      "source": [
        "mu.plot([time, time[tau:], time[n_train + tau:]],\n",
        "         [mu.numpy(x), mu.numpy(onestep_preds),\n",
        "          mu.numpy(multistep_preds[n_train + tau:])], 'time',\n",
        "         'x', legend=['data', '1-step preds', 'multistep preds'],\n",
        "         xlim=[1, 1000], figsize=(6, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl0MvEeUanyk"
      },
      "source": [
        "# Answer\n",
        "\n",
        "* Why did the algorithm work so poorly?\n",
        "* This is ultimately due to the fact that the errors build up. Let us say that after step 1 we have some error $\\epsilon_1 = \\bar\\epsilon$. Now the *input* for step 2 is perturbed by $\\epsilon_1$, hence we suffer some error in the order of $\\epsilon_2 = \\bar\\epsilon + c \\epsilon_1$ for some constant $c$, and so on. The error can diverge rather rapidly from the true observations. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
