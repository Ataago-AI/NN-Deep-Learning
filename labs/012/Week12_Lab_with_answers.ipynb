{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Raw Cell Format","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"EzCos2IunsdM"},"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elyRUwxtnqYK"},"source":["%matplotlib inline\n","import my_utils as mu\n","import math\n","import torch\n","from torch import nn\n","from torch.nn import functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AfGnTDU-vdbE"},"source":["# The Task\n","\n","* Our **Task** for this week is to the implement GRU layer, one of the most widely-used layers for Recurrent Neural Networks!\n","* The only task that you have to do is to write the code for the GRU layer (see below). The rest of the training pipeline is provided to you.\n","* The GRU layer is described by the following figure and equations."]},{"cell_type":"markdown","metadata":{"id":"-n0sfUK0nqYj"},"source":["# GRU Schematic Diagram\n","<!-- \n","![ Hidden state computation in a GRU. As before, the multiplication is carried out elementwise. ](img/gru_3.svg) -->\n","\n","![ Hidden state computation in a GRU. As before, the multiplication is carried out elementwise. ](https://drive.google.com/uc?export=view&id=1_ZTvSMWMGvQruQent0FRKIgJ_7m9YqPE)  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y_-AH-WUvdbF"},"source":["# GRU Equations\n","\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n","\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n","\\end{aligned}\n","$$\n","\n","and\n","\n","$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$\n","\n","and finally:\n","\n","$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x5nk5B8_nqYl"},"source":["# GRU Implementation from Scratch\n"]},{"cell_type":"markdown","metadata":{"id":"d_Z5m3O9vdbG"},"source":["# Loading the Dataset"]},{"cell_type":"code","metadata":{"id":"iTumqFCznqYm","origin_pos":2,"tab":["pytorch"]},"source":["batch_size, num_steps = 32, 35\n","train_iter, vocab = mu.load_data_time_machine(batch_size, num_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2E6qzzN6nqYm","origin_pos":3},"source":["# Initializing Model Parameters\n","\n","* The weights are drawn from a Gaussian with standard deviation to be $0.01$ and set the bias to $0$. \n","* The hyperparameter `num_hiddens` defines the number of hidden units. \n","* Weights and biases are instantiated for the update gate, the reset gate, and the candidate hidden state. \n","* Gradients are attached to all the parameters."]},{"cell_type":"code","metadata":{"id":"vP6idgoenqYn","origin_pos":5,"tab":["pytorch"]},"source":["def get_params(vocab_size, num_hiddens):\n","    num_inputs = num_outputs = vocab_size\n","\n","    def normal(shape):\n","        return torch.randn(size=shape)*0.01\n","\n","    def three():\n","        return (normal((num_inputs, num_hiddens)),\n","                normal((num_hiddens, num_hiddens)),\n","                torch.zeros(num_hiddens))\n","\n","    W_xz, W_hz, b_z = three()  # Update gate parameter\n","    W_xr, W_hr, b_r = three()  # Reset gate parameter\n","    W_xh, W_hh, b_h = three()  # Candidate hidden state parameter\n","    # Output layer parameters\n","    W_hq = normal((num_hiddens, num_outputs))\n","    b_q = torch.zeros(num_outputs)\n","    # Attach gradients\n","    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n","    for param in params:\n","        param.requires_grad_(True)\n","    return params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASDqRfbwnqYo","origin_pos":6},"source":["# Defining the Model"]},{"cell_type":"code","metadata":{"id":"D8bdDGJYnqYo","origin_pos":8,"tab":["pytorch"]},"source":["# Hidden state init. function returns a tensor with zeros of shape (batch size, number of hidden units) \n","def init_gru_state(batch_size, num_hiddens):\n","    return (torch.zeros((batch_size, num_hiddens)), )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kLH5N0D0vdbI"},"source":["# Your Task: \n","\n","* Implement GRU layer according to GRU Schematic Diagram and GRU Equations"]},{"cell_type":"code","metadata":{"id":"Hr7ngBWenqYp","origin_pos":11,"tab":["pytorch"]},"source":["# for our example inputs is a tensor of dims 35 x 32 x 28, state is a tensor of dims 32 x 256, and \n","# params=get_params(28, 256) are the model params\n","# The layer should return Y and new state H\n","\n","def gru(inputs, state, params):\n","    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    for X in inputs:\n","        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n","        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n","        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n","        H = Z * H + (1 - Z) * H_tilda\n","        Y = H @ W_hq + b_q\n","        outputs.append(Y)\n","    Y = torch.cat(outputs, dim=0)\n","    return Y, (H,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNLIvF5bvdbJ"},"source":["class RNNModelScratch:  \n","    \"\"\"A RNN Model implemented from scratch.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, get_params,\n","                 init_state, forward_fn):\n","        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n","        self.params = get_params(vocab_size, num_hiddens)\n","        self.init_state, self.forward_fn = init_state, forward_fn\n","\n","    def __call__(self, X, state):\n","        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n","        #Y,  H = self.forward_fn(X, state, self.params)\n","        #print(Y.size(), H.size())\n","        return self.forward_fn(X, state, self.params)\n","\n","    def begin_state(self, batch_size, device):\n","        return self.init_state(batch_size, self.num_hiddens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BiS2dyuxnqYq","origin_pos":12},"source":["# Training and Prediction\n","\n","* Training and prediction work are the same as in basic RNN. "]},{"cell_type":"code","metadata":{"id":"Sa1kqelznqYq","origin_pos":13,"tab":["pytorch"]},"source":["vocab_size, num_hiddens = len(vocab), 256\n","num_epochs, lr = 500, 1\n","model = RNNModelScratch(len(vocab), num_hiddens, get_params, init_gru_state, gru)\n","mu.train_ch8(model, train_iter, vocab, lr, num_epochs, device='cpu')"],"execution_count":null,"outputs":[]}]}