{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Tensor_Data_Processing.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnIXFT3hbQhz","executionInfo":{"status":"ok","timestamp":1610894012496,"user_tz":0,"elapsed":23292,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"50cfb834-4312-4fed-b9bf-65d89577ef18"},"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"u528q8RCbQh6"},"source":["# Data Manipulation\n","\n","* Basic data structure used in Deep Learning is the $n$-dimensional array, which is also called the *tensor*.\n","* *Tensor class* is called `Tensor` in PyTorch and is similar to NumPy's `ndarray` with a few killer features.\n","    * First, GPU is well-supported to accelerate the computation\n","    * Second, the tensor class supports automatic differentiation.\n","* These properties make the tensor class suitable for Deep Learning.\n"]},{"cell_type":"code","metadata":{"origin_pos":5,"scrolled":true,"tab":["pytorch"],"id":"LKSF3PUXbQh7","executionInfo":{"status":"ok","timestamp":1610894019403,"user_tz":0,"elapsed":2966,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["# To start, we import `torch`. Note that it's called PyTorch, we should import `torch` instead of `pytorch`.\n","import torch"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dRE9OyLbQh8","executionInfo":{"status":"ok","timestamp":1610894020549,"user_tz":0,"elapsed":490,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"7b06ce6e-ee6a-4ef0-9828-a690b4cae630"},"source":["print(torch.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":7,"id":"rGvyZmEwbQh9"},"source":["# Tensor\n","\n","* A tensor represents a (possibly multi-dimensional) array of numerical values.\n","    * A 1D tensor corresponds (in math) to a *vector*.\n","    * A 2D tensor corresponds to a *matrix*.\n","    * Tensors with more than two axes do not have special mathematical names.\n"]},{"cell_type":"markdown","metadata":{"id":"Oy1peA3MbQh9"},"source":["# Vector\n","\n","* In math notation, we will usually denote vectors as bold-faced, lower-cased letters (e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n","* Column vectors is the default orientation of vectors. In math, a column vector $\\mathbf{x}$ can be written as\n","\n","$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n","where $x_1, \\ldots, x_n$ are elements of the vector."]},{"cell_type":"code","metadata":{"origin_pos":9,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"kxAdxaKcbQh9","executionInfo":{"status":"ok","timestamp":1610894026140,"user_tz":0,"elapsed":933,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"89c7e078-3829-4629-c490-814ddaa1bec9"},"source":["# A vector with 4 elements in the range 0-3\n","# Unless otherwise specified, a new tensor is stored in main memory and designated for CPU-based computation\n","x = torch.arange(4)\n","print(type(x))\n","print(x)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","tensor([0, 1, 2, 3])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeQa40RabQh-","executionInfo":{"status":"ok","timestamp":1610894027503,"user_tz":0,"elapsed":517,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"da21206b-35f8-4ad1-fa51-296f925b7ef2"},"source":["# access the i-th element: x[i]\n","print(x[3])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor(3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"origin_pos":11,"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"ZI2FuIqebQh-","executionInfo":{"status":"ok","timestamp":1610894028929,"user_tz":0,"elapsed":478,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"ee0a9a24-b447-4959-ff0f-0564d1869e7d"},"source":["# Vector shape i.e. dimensionality \n","print(len(x), x.size(), x.shape, type(x.size()))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["4 torch.Size([4]) torch.Size([4]) <class 'torch.Size'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TUb8DVCUbQh_"},"source":["# Matrices\n","\n","* Matrices (i.e. 2D tensors) will be typically denoted with bold-faced, capital letters (e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$).\n","\n","* In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars.\n","\n","* Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table,\n","where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n","\n","$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n","\n","* For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$ is ($m$, $n$) or $m \\times n$.\n","    * When a matrix has the same number of rows and columns, it is called a *square matrix*.\n"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8U9keq5sbQh_","executionInfo":{"status":"ok","timestamp":1610894033553,"user_tz":0,"elapsed":529,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"59a42ac0-5a08-4b7e-ab39-14ccbc4a3311"},"source":["# Reshape function: change the shape of a tensor without changing the number of elements or their values \n","A = torch.arange(20).reshape(5, 4)\n","A, A[2, 3], A[2][3]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11],\n","         [12, 13, 14, 15],\n","         [16, 17, 18, 19]]), tensor(11), tensor(11))"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"HMIwWi0ZbQh_","executionInfo":{"status":"ok","timestamp":1610894034605,"user_tz":0,"elapsed":383,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"638bf951-9d93-40a0-dfb9-fb385ef94ed9"},"source":["# Matrix shape\n","print(len(A), A.size(), A.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["5 torch.Size([5, 4]) torch.Size([5, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfnqT2DJbQiA","executionInfo":{"status":"ok","timestamp":1610894036020,"user_tz":0,"elapsed":509,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"087063fa-e83d-45db-e49b-5799ca5d50c8"},"source":["# use -1 for the dimension that can be automatically inferred\n","A1 = torch.arange(20).reshape(5, -1);\n","A2 = torch.arange(20).reshape(-1, 4);\n","A==A1"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gq8STO4VbQiA","executionInfo":{"status":"ok","timestamp":1610894037360,"user_tz":0,"elapsed":602,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"2f89dfce-661c-4fd1-e164-6dfef21be2e3"},"source":["# Transpose\n","B = A.T\n","B1 = A.transpose(1, 0)\n","B, B1"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0,  4,  8, 12, 16],\n","         [ 1,  5,  9, 13, 17],\n","         [ 2,  6, 10, 14, 18],\n","         [ 3,  7, 11, 15, 19]]), tensor([[ 0,  4,  8, 12, 16],\n","         [ 1,  5,  9, 13, 17],\n","         [ 2,  6, 10, 14, 18],\n","         [ 3,  7, 11, 15, 19]]))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"1S2vzvhybQiB"},"source":["# Tensors"]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"IBVTYkptbQiB","executionInfo":{"status":"ok","timestamp":1610894041781,"user_tz":0,"elapsed":754,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"c3189462-0df2-4f92-8a4d-ad4892c4536f"},"source":["# A 3D tensor\n","X = torch.arange(24).reshape(2, 3, -1)\n","X"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"yGz3gmpXbQiB"},"source":["# Commonly-used Tensor Constuctors\n"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"_dhFpD5sbQiB","executionInfo":{"status":"ok","timestamp":1610894045805,"user_tz":0,"elapsed":475,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"e9e82f6b-839e-4759-eace-afeedb89e8be"},"source":["torch.ones((2, 3, 4)) # with Ones"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEi7T-_qbQiC","executionInfo":{"status":"ok","timestamp":1610894046857,"user_tz":0,"elapsed":368,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"2864536a-4075-4f02-f2e6-df0192e6d5ef"},"source":["torch.zeros(2, 3) # with Zeros"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Tof2tVtGbQiC","executionInfo":{"status":"ok","timestamp":1610894047887,"user_tz":0,"elapsed":487,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"c518c5b4-5e90-40ba-c89e-6036aac12d04"},"source":["torch.randn(3, 4) # samples from a Gaussian distribution with mean 0 and std of 1"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1048, -0.4926, -0.5236, -0.9925],\n","        [ 0.9997, -0.3656, -1.4240,  0.1609],\n","        [ 0.7900, -0.9788,  2.1395,  0.9936]])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUy5p6A6bQiC","executionInfo":{"status":"ok","timestamp":1610894048965,"user_tz":0,"elapsed":537,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"6f6b26c4-18ec-4b21-8fdc-ab0366795c74"},"source":["torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4]]) # From Python lists "],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 1, 4, 3],\n","        [1, 2, 3, 4]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"LFZreKf2bQiD"},"source":["# Common Tensor Operators"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp_a4tGObQiD","executionInfo":{"status":"ok","timestamp":1610894055253,"user_tz":0,"elapsed":896,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"67845a13-004c-4caa-d52f-67011ea84c35"},"source":["A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n","A, A + B, A * B, A + 2 "],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],\n","         [ 8., 10., 12., 14.],\n","         [16., 18., 20., 22.],\n","         [24., 26., 28., 30.],\n","         [32., 34., 36., 38.]]), tensor([[  0.,   1.,   4.,   9.],\n","         [ 16.,  25.,  36.,  49.],\n","         [ 64.,  81., 100., 121.],\n","         [144., 169., 196., 225.],\n","         [256., 289., 324., 361.]]), tensor([[ 2.,  3.,  4.,  5.],\n","         [ 6.,  7.,  8.,  9.],\n","         [10., 11., 12., 13.],\n","         [14., 15., 16., 17.],\n","         [18., 19., 20., 21.]]))"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_8TN5WKbQiD","executionInfo":{"status":"ok","timestamp":1610894056727,"user_tz":0,"elapsed":483,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"27026c09-a829-4d78-e410-58251280ab67"},"source":["# Summations (same applies for mean() function)\n","A.sum(), A.sum(dim=0), A.sum(dim=1)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(190.), tensor([40., 45., 50., 55.]), tensor([ 6., 22., 38., 54., 70.]))"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"E9aakDknbQiE","executionInfo":{"status":"ok","timestamp":1610894057717,"user_tz":0,"elapsed":484,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"0330bcdb-a5f0-4c83-c2d7-a1c32ef6b2cb"},"source":["# Functions are applied element-wise\n","torch.exp(A), A**2, torch.pow(A, 2), torch.cos(A)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01],\n","         [5.4598e+01, 1.4841e+02, 4.0343e+02, 1.0966e+03],\n","         [2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04],\n","         [1.6275e+05, 4.4241e+05, 1.2026e+06, 3.2690e+06],\n","         [8.8861e+06, 2.4155e+07, 6.5660e+07, 1.7848e+08]]),\n"," tensor([[  0.,   1.,   4.,   9.],\n","         [ 16.,  25.,  36.,  49.],\n","         [ 64.,  81., 100., 121.],\n","         [144., 169., 196., 225.],\n","         [256., 289., 324., 361.]]),\n"," tensor([[  0.,   1.,   4.,   9.],\n","         [ 16.,  25.,  36.,  49.],\n","         [ 64.,  81., 100., 121.],\n","         [144., 169., 196., 225.],\n","         [256., 289., 324., 361.]]),\n"," tensor([[ 1.0000,  0.5403, -0.4161, -0.9900],\n","         [-0.6536,  0.2837,  0.9602,  0.7539],\n","         [-0.1455, -0.9111, -0.8391,  0.0044],\n","         [ 0.8439,  0.9074,  0.1367, -0.7597],\n","         [-0.9577, -0.2752,  0.6603,  0.9887]]))"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"AFwrFy6YbQiE","executionInfo":{"status":"ok","timestamp":1610894058903,"user_tz":0,"elapsed":533,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"b73d5b44-d491-4776-fa40-64cc1d763cb1"},"source":["# Concatenation\n","torch.cat((A, B), dim=0), torch.cat((A, B), dim=1)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.],\n","         [ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.]]),\n"," tensor([[ 0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.,  8.,  9., 10., 11.],\n","         [12., 13., 14., 15., 12., 13., 14., 15.],\n","         [16., 17., 18., 19., 16., 17., 18., 19.]]))"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"MQvDz7TqbQiE"},"source":["# Dot Products\n","\n","* One of the most fundamental operations. \n","* Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their *dot product* $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$) is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VMPW6RmbQiE","executionInfo":{"status":"ok","timestamp":1610894061260,"user_tz":0,"elapsed":465,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"7a392b67-dbae-4e50-a36c-3507646c8609"},"source":["x = torch.arange(4, dtype=torch.float32)\n","y = torch.ones(4, dtype = torch.float32)\n","x, y, torch.dot(x, y)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"qlrjbcofbQiF"},"source":["* Dot products are useful in a wide range of contexts:\n","    1. Given features stored in vector $\\mathbf{x}  \\in \\mathbb{R}^d$ and model weights in vector $\\mathbf{w} \\in \\mathbb{R}^d$, the *score* between features and model weights are given by $\\mathbf{x}^\\top \\mathbf{w}$.\n","    2. When the weights are non-negative and sum to one (i.e., $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$), the dot product expresses a *weighted average*.\n","    3. After normalizing two vectors to have the unit length (to be defined below), the dot products express the cosine of the angle between them.\n"]},{"cell_type":"markdown","metadata":{"id":"WWkx1OVQbQiF"},"source":["# Matrix-Vector Products\n","\n","* Recall $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{x} \\in \\mathbb{R}^n$. Let us write $\\mathbf{A}$ in terms of its row vectors:\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix},$$\n","where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n","\n","* $\\mathbf{A}\\mathbf{x}$ is a column vector of length $m$, whose $i^\\mathrm{th}$ element is $\\mathbf{a}^\\top_i \\mathbf{x}$:\n","\n","$$\n","\\mathbf{A}\\mathbf{x}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix}\\mathbf{x}\n","= \\begin{bmatrix}\n"," \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n"," \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n","\\vdots\\\\\n"," \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n","\\end{bmatrix}.\n","$$\n","\n","* Multiplication by $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ projects vectors from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlHXImeIbQiF","executionInfo":{"status":"ok","timestamp":1610894065067,"user_tz":0,"elapsed":443,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"d88cd6cc-a8f2-44e1-8f72-bec8a287eb06"},"source":["A.shape, x.shape, torch.mv(A, x)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"I4WilX3FbQiF"},"source":["# Matrix-Matrix Multiplication\n","\n","\n","* Asssume that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n","\\end{bmatrix}.$$\n","\n","\n","* Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ the row vector representing the $i^\\mathrm{th}$ row of $\\mathbf{A}$, and by $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ the column vector from the $j^\\mathrm{th}$ column of $\\mathbf{B}$. We write $\\mathbf{A}$ and $\\mathbf{B}$ as:\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix},\n","\\quad \\mathbf{B}=\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}.\n","$$\n","\n","\n","* Then the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is:\n","\n","$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n"," \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n"," \\vdots & \\vdots & \\ddots &\\vdots\\\\\n","\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n","\\end{bmatrix}.\n","$$\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yft9jH64bQiG","executionInfo":{"status":"ok","timestamp":1610894068157,"user_tz":0,"elapsed":430,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"22592069-2dfb-4403-e6b2-d36711f57c51"},"source":["# Matrix multiplication\n","B = torch.ones(4, 3)\n","A, B, torch.mm(A, B)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.]]), tensor([[1., 1., 1.],\n","         [1., 1., 1.],\n","         [1., 1., 1.],\n","         [1., 1., 1.]]), tensor([[ 6.,  6.,  6.],\n","         [22., 22., 22.],\n","         [38., 38., 38.],\n","         [54., 54., 54.],\n","         [70., 70., 70.]]))"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"B1ewx1oBbQiH"},"source":["# Norms\n","\n","* Some of the most useful operators in linear algebra are *norms*.\n","* Informally, the norm of a vector tells us how *big* a vector is (0 is the minimum). \n","* A (vector) norm is a function $f$ that maps a vector to a scalar, satisfying the following properties:\n","    1. $f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$\n","\n","    2. $f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$\n","\n","    3. $f(\\mathbf{x}) \\geq 0.$\n","\n","    4. $\\forall i, x_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$"]},{"cell_type":"markdown","metadata":{"id":"sLvYSJkRbQiH"},"source":["* The $L_2$ *norm* of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:\n","$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$\n","where the subscript $2$ is often omitted in $L_2$ norms, i.e., $\\|\\mathbf{x}\\|$ is equivalent to $\\|\\mathbf{x}\\|_2$. \n","\n","* The $L_1$ *norm* is expressed as the sum of the absolute values of the vector elements:\n","\n","$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$\n","\n","* In Deep Learning, we are often trying to solve optimization problems: e.g. *minimize* the distance between the model's predictions and the ground-truth observations.\n","    * The optimization obectives are ofter expressed as norms."]},{"cell_type":"markdown","metadata":{"origin_pos":53,"id":"AUfy0xr5bQiH"},"source":["# Broadcasting Mechanism\n","\n","* In maths, in order to to perform elementwise operations between two tensors, they need to have the same shape. \n","* In Python and PyTorch, under certain conditions, even when their shapes differ, we can still perform elementwise operations by invoking the *broadcasting mechanism*.\n","* This mechanism works in the following way: \n","    * First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape.\n","    * Second, carry out the elementwise operations on the resulting arrays.\n","\n","* In most cases, we broadcast along a dimension where an array initially only has length 1, such as in the following example.\n"]},{"cell_type":"code","metadata":{"origin_pos":55,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"MI5nPGrnbQiH","executionInfo":{"status":"ok","timestamp":1610894082982,"user_tz":0,"elapsed":471,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"d70c3915-e3d1-466f-86e9-570899559581"},"source":["a = torch.arange(3).reshape((3, 1))\n","b = torch.arange(2).reshape((1, 2))\n","a, b, a.size(), b.size()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0],\n","         [1],\n","         [2]]), tensor([[0, 1]]), torch.Size([3, 1]), torch.Size([1, 2]))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"origin_pos":58,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"OF3rrJd9bQiI","executionInfo":{"status":"ok","timestamp":1610894084781,"user_tz":0,"elapsed":651,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"6f9c68c3-02e3-4b30-8d8a-ef0ad3479bcb"},"source":["a + b"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"67G-yuhxbQiI","executionInfo":{"status":"ok","timestamp":1610894086159,"user_tz":0,"elapsed":495,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["# Another example with sums\n","A = torch.arange(20, dtype=torch.float32).reshape(5, 4)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoWThrfpbQiJ","executionInfo":{"status":"ok","timestamp":1610894088639,"user_tz":0,"elapsed":528,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"74329979-3603-41af-dc99-2700f1632cf4"},"source":["B1 = A.sum(dim=1);\n","B2 = A.sum(dim=1, keepdims=True);\n","B1, B2"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 6., 22., 38., 54., 70.]), tensor([[ 6.],\n","         [22.],\n","         [38.],\n","         [54.],\n","         [70.]]))"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yf77hLrpbQiJ","executionInfo":{"status":"ok","timestamp":1610894091884,"user_tz":0,"elapsed":496,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"34c2da53-e7ec-4efa-a536-f09abe94cf65"},"source":["# Dimensionality\n","B1.size(), B2.size()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5]), torch.Size([5, 1]))"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BobOi9e3bQiK","executionInfo":{"status":"ok","timestamp":1610894093214,"user_tz":0,"elapsed":450,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"d04a1865-0f75-4baf-fc03-920bb0352b69"},"source":["A/B2 # A/B1 won't work"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n","        [0.1818, 0.2273, 0.2727, 0.3182],\n","        [0.2105, 0.2368, 0.2632, 0.2895],\n","        [0.2222, 0.2407, 0.2593, 0.2778],\n","        [0.2286, 0.2429, 0.2571, 0.2714]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"origin_pos":59,"id":"KFP1LNMDbQiK"},"source":["# Tensor Indexing and Slicing\n","\n","* Just as in any other Python array, elements in a tensor can be accessed by index.\n","    * The first element has index 0 and ranges are specified to include the first but *before* the last element.\n","    * As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices.\n","    * Example: `[-1]` selects the last element and `[1:3]` selects the second and the third elements as follows:\n"]},{"cell_type":"code","metadata":{"origin_pos":60,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"gvD7dGIrbQiL","executionInfo":{"status":"ok","timestamp":1610894096317,"user_tz":0,"elapsed":430,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"fab1c1d7-14b0-4c48-d8f8-70e00c06afee"},"source":["X = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","X, X[1:3, :], X[1:3, :2],  X[-1, :], X[-2:-1, :] "],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.]]), tensor([[ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.]]), tensor([[4., 5.],\n","         [8., 9.]]), tensor([16., 17., 18., 19.]), tensor([[12., 13., 14., 15.]]))"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"origin_pos":68,"id":"G1ua9S-KbQiL"},"source":["# Saving Memory\n","\n","* Running operations can cause new memory to be allocated to store the results.\n","* We do not want to allocate memory unnecessarily all the time.\n","    * In machine learning, we might have hundreds of megabytes of parameters\n","* Where possible, we want to perform these updates *in place*."]},{"cell_type":"code","metadata":{"origin_pos":69,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"C5HAGjd9bQiL","executionInfo":{"status":"ok","timestamp":1610894099219,"user_tz":0,"elapsed":442,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"2fa8888c-3312-4661-dc26-bd5cc9164b14"},"source":["# In place example\n","X = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","Y = 10*X\n","print(id(X), id(Y))\n","Y = Y + X # not in-place\n","print(id(X), id(Y))\n","Y += X # in-place\n","print(id(X), id(Y))\n","Y[:] = Y + X # in-place\n","print(id(X), id(Y))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["140377999456584 140377999456512\n","140377999456584 140377999432944\n","140377999456584 140377999432944\n","140377999456584 140377999432944\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":80,"id":"IPZ7SUmybQiM"},"source":["# Conversion to Other Python Objects\n"]},{"cell_type":"code","metadata":{"origin_pos":82,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"iGMYuFTsbQiM","executionInfo":{"status":"ok","timestamp":1610894101692,"user_tz":0,"elapsed":475,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"f3d79b90-ac32-421b-8cb6-09372635b590"},"source":["# Converting to a NumPy array, or vice versa\n","A = X.numpy()\n","B = torch.tensor(A)\n","type(A), type(B)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(numpy.ndarray, torch.Tensor)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"origin_pos":86,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"bdBl_j_AbQiM","executionInfo":{"status":"ok","timestamp":1610894102993,"user_tz":0,"elapsed":586,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"dca06836-afd2-4a07-a134-994767860aec"},"source":["# Converting to a size-1 tensor to a Python scalar,\n","a = torch.tensor([3.5])\n","a, a.item(), float(a), int(a)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3.5000]), 3.5, 3.5, 3)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"6xaA19TIxU8f"},"source":[""],"execution_count":null,"outputs":[]}]}