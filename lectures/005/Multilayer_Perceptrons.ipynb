{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ueLz1ok_M4r8"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsU1mA8zM4sD"},"outputs":[],"source":["%matplotlib inline\n","import my_utils as mu\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"F-BIkyK0M4sE"},"source":["# Multilayer Perceptrons\n","\n","* Last week: we introduced and implemented softmax regression to recognize 10 categories of clothing from images.\n","    * This model mapped inputs directly to outputs via a single linear transformation, followed by a softmax operation.\n","* Now that we have mastered the mechanics of simple linear models, we can launch our exploration of deep neural networks"]},{"cell_type":"markdown","metadata":{"id":"gUxy3rGsM4sF"},"source":["\n","# Linear Models May Go Wrong\n","\n","* If labels truly related to our input data by a linear transformation, then a linear model would be sufficient.\n","* But linearity  is a *strong* assumption.\n","\n","## Examples\n","\n","* **Example 1:** Predict whether an individual will repay a loan.\n","  * Use income as input feature\n","  * An individual with a higher income would be more likely to repay (than one with a lower income).\n","  * However, this relationship is not linearly associated with the probability of repayment. \n","    * An increase in income from 0 to 50 thousand likely corresponds to a bigger increase in likelihood of repayment than an increase from 1 million to 1.05 million."]},{"cell_type":"markdown","metadata":{"id":"QktteWyxiEpu"},"source":["# Feature Extraction\n","\n","* One way to handle this might be to **preprocess** input data such that linearity is plausible; e.g. by using the logarithm of income as our feature.\n","  * Preprocess input data $\\triangleq$ extract features from input data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GGFDYLCGM4sF"},"source":["\n","* **Example 2:** Classify images of cats and dogs.\n","    * Relying on a linear model implicit assumes that for differentiating cats vs. dogs can be done using only the brightness of individual pixels.\n","    * Doesn't always work well\n","    * Less obvious that we could address the problem with a simple preprocessing fix.\n","\n","* We can use deep neural networks, to jointly learn (from input data) both to extract features from the data and a classifier that acts upon the features."]},{"cell_type":"markdown","metadata":{"id":"TqWfuQZ7M4sG"},"source":["# Incorporating Hidden Layers\n","\n","* We overcome the limitations of linear models and handle a more general class of functions by incorporating one or more hidden layers.\n","* The easiest way to do this is to stack many fully-connected (linear) layers on top of each other.\n","* Each layer feeds into the layer above it, until we generate outputs.\n","* This architecture is commonly called a **Multi-Layer Perceptron** (**MLP**).\n","\n","<!-- ![An MLP with a hidden layer of 5 hidden units.](img/mlp.svg)  -->\n","\n","![An MLP with a hidden layer of 5 hidden units.](https://drive.google.com/uc?export=view&id=1Q6UBzSCH5JMovX9xKJhKY6SuNKwDFEMv) \n","\n","* The above MLP has 4 inputs, 3 outputs, and its hidden layer contains 5 hidden units.\n","* The number of layers in this MLP is 2.\n","    * The last (second) layer is the classifier.\n","    * The input to the last (second) layer are the **features**. \n","        * So the first layer calculates features from the input data.\n"]},{"cell_type":"markdown","metadata":{"id":"3159sZfMM4sG"},"source":["# The Maths of the Model\n","\n","* The outputs $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ of the previous one-hidden-layer MLP are calculated from:\n","$$\n","\\begin{aligned}\n","    \\mathbf{H} & = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}, \\\\\n","    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}, \n","\\end{aligned}\n","$$\n","   * where the matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, denotes a minibatch of $n$ examples with $d$ inputs (features).\n","   * $\\mathbf{X}$ is processed by the first hidden linear layer having weights $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$ and biases $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$\n","   * The output of the hidden layer (i.e. the extracted features) is denoted by $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$.\n","   * $\\mathbf{H}$ is processed by the second linear layer having weights $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$ and biases $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XFcQHPLsM4sH","origin_pos":0},"source":["# Equivalence to a Linear Model\n","\n","* The hidden units above are given by a linear function of the inputs, and the outputs (pre-softmax) are just a linear function of the hidden units.\n","* Overall this is still a linear model with 2 layers\n","* It is equivalent to a single-layer linear model with parameters \n","\n","$$\\mathbf{W} = \\mathbf{W}^{(1)}\\mathbf{W}^{(2)},$$  \n","$$\\mathbf{b} = \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.$$\n","\n","* Proof:\n","\n","$$\n","\\mathbf{O} = (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W}^{(1)}\\mathbf{W}^{(2)} + \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}.\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"GOuzS1zYM4sH"},"source":["# Adding Non-linearities\n","\n","* To realize the potential of multilayer architectures, we need one more key ingredient: a **nonlinear activation function** $\\sigma$ to be applied to each hidden unit after the linear transformation.\n","* With activation functions in place, it is no longer possible to collapse an MLP into a linear model:\n","$$\n","\\begin{aligned}\n","    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n","    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n","\\end{aligned}\n","$$\n","\n","* Typically, the activation functions are applied elementwise.\n","* The outputs of activation functions are called **activations** or **features**.\n","* To build more general MLPs, we can continue stacking such hidden layers, e.g., $\\mathbf{H}^{(1)} = \\sigma_1(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$ and $\\mathbf{H}^{(2)} = \\sigma_2(\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$, one on top of the other, yielding ever more expressive models.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BuptLpfEM4sH"},"source":["# Universal Approximators\n","\n","* MLPs can capture complex interactions among their inputs via their hidden neurons.\n","* For example, we can easily design hidden nodes to perform basic logic operations on a pair of inputs.\n","* Moreover, for certain choices of the activation function, it is widely known that MLPs are universal approximators.\n","    * Even with a single-hidden-layer network, given enough nodes, and the right set of weights, we can model any function\n","    * However, actually finding the weights is the hard part.\n","* We can learn functions a lot more easily from data by using deep networks.\n"]},{"cell_type":"markdown","metadata":{"id":"q1Va1rdGM4sI","origin_pos":4},"source":["# Activation Functions\n","* Activation functions decide whether a neuron should be activated. \n","* They are differentiable operators. \n","\n","## ReLU Function\n","* The most popular choice due to its great performance on a variety of predictive tasks, is the **Rectified Linear Unit (ReLU)**.\n","* ReLU provides a very simple nonlinear transformation. It is defined as:\n","$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n","* ReLU  retains only positive elements and `discards' all negative elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtHGzez7M4sJ","origin_pos":6,"tab":["pytorch"]},"outputs":[],"source":["x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n","y = torch.relu(x)\n","mu.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"]},{"cell_type":"markdown","metadata":{"id":"oaUR2_o7M4sK","origin_pos":8},"source":["## ReLU Function\n","\n","* When $x<0$, the derivative of the ReLU is 0, and when $x>0$, it is equal to 1.\n","* ReLU  is not differentiable when $x=0$: we just set the derivative to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzdd5nL4M4sK","origin_pos":10,"scrolled":true,"tab":["pytorch"]},"outputs":[],"source":["y.backward(torch.ones_like(x), retain_graph=True)\n","mu.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"]},{"cell_type":"markdown","metadata":{"id":"D2uPlMOnM4sL"},"source":["* ReLU works well because its derivatives are well-behaved: either they vanish or they let the argument through.\n","* This makes optimization better behaved and mitigates the known problem of vanishing gradients \n","* *Parameterized ReLU* (*pReLU*) allows information to flow, even when $x<0$:\n","\n","$$\\operatorname{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x).$$"]},{"cell_type":"markdown","metadata":{"id":"bg_tn_c5M4sL","origin_pos":12},"source":["\n","## Sigmoid Function\n","\n","* The **sigmoid function** transforms its input to output that lies on the interval (0, 1):\n","    * Also called a *squashing function*\n","\n","$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n","\n","* An approximation to step function used to model biological neurons which either *fire* or *do not fire*.\n","* In constrast to step function, the sigmoid is smooth, differentiable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iu86NcA3M4sL","origin_pos":14,"tab":["pytorch"]},"outputs":[],"source":["y = torch.sigmoid(x)\n","mu.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))"]},{"cell_type":"markdown","metadata":{"id":"jNXc22ggM4sM","origin_pos":16},"source":["## Sigmoid Function\n","\n","* The derivative of the sigmoid function is given by :\n","\n","$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n","\n","* As the input diverges from 0 in either direction, the derivative approaches 0.\n","    * Compare this to ReLU!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA9TGsh-M4sM","origin_pos":18,"tab":["pytorch"]},"outputs":[],"source":["# Sigmoid derivative\n","x.grad.data.zero_()\n","y.backward(torch.ones_like(x),retain_graph=True)\n","mu.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))"]},{"cell_type":"markdown","metadata":{"id":"wclBA2E7M4sM","origin_pos":20},"source":["# Tanh Function\n","\n","* Similar to sigmoid. Also squashes its inputs to $\\left[-1, 1\\right]$:\n","\n","$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n","\n","* The tanh function exhibits point symmetry about the origin of the coordinate system.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpcoqCgTM4sM","origin_pos":22,"tab":["pytorch"]},"outputs":[],"source":["y = torch.tanh(x)\n","mu.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))"]},{"cell_type":"markdown","metadata":{"id":"PWTzN_P7M4sN","origin_pos":24},"source":["* The derivative of the tanh function is: $\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$\n","\n","* Similar shape to the derivative of the sigmoid. \n"]},{"cell_type":"markdown","metadata":{"id":"KhDM41WiM4sN","origin_pos":28},"source":["\n","# Summary\n","\n","* MLP adds one or multiple fully-connected hidden layers between the output and input layers and transforms the output of the hidden layer via an activation function.\n","* Commonly-used activation functions include the ReLU function, the sigmoid function, and the tanh function.\n","\n","\n","\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}