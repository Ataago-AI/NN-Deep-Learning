{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qFY8uG2gW02n"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfxYevkNW02v"},"outputs":[],"source":["import torch\n","from torch import nn\n","import my_utils as mu"]},{"cell_type":"markdown","metadata":{"id":"FrMzeQ_LiGHm"},"source":["# Optimization Vs. Generalization\n","\n","* To find the best parameters for our model, we mimimize the average loss over all training examples.\n","    * This is an optimization problem.\n","* Minimizing the training loss is **not** however our ultimate objective:\n","    * What we want is our model is to **generalize** i.e. to exhibit good accuracy on a seperate validation/test set.\n","* The following figure show this (note we plot the error = 1 - accuracy):    \n","\n","<img src=\"img/train-val-error.png\" alt=\"drawing\" width=\"350\"/> \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FVOp9Zk_W02v"},"source":["# Checking with a Validation Set\n","\n","\n","* The final solution depends on:\n","    * the **initial values of the parameters**.\n","    * the **optimization algorithm** chosen (e.g. SGD, Adam).\n","    * the **hyper-parameters** chosen (e.g. batch-size, learning rate).\n","* Also we want to avoid overfitting:\n","    * We can use **regularization**.\n","\n","* All of the above need to be chosen accordingly every time we train a neural network for a new task.\n","    * The **architecture** is also important (will not be covered here).\n","        \n","* Some choices can be checked by monitoring the training loss. \n","* However, we always need to check using a **seperate validation set**. "]},{"cell_type":"markdown","metadata":{"id":"YdHf7LV1W02w"},"source":["# Parameter Initialization\n","\n","\n","## Default Initialization\n","\n","* Previously we used a normal distribution to initialize the values of our weights.\n","* If we do not specify the initialization method, PyTorch will use a default random initialization method, which often works well in practice\n","    * He initialization \n","* `torch.nn.init` package provides several ways to init the parameters. "]},{"cell_type":"markdown","metadata":{"id":"5ufyx0fiW02y"},"source":["## Xavier Initialization\n"," \n","* As illustrated by Glorot and Bengio, a good initialization of the weights could be the one such that the values of the input features and the output features are in the same range.\n","* To achieve this, Xavier initialization initializes weights from a Gaussian with zero mean and variance\n","$\\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}$.\n","* PyTorch implementation: `torch.nn.init.xavier_normal_`\n","* Xavier's initialization can be applied when sampling the weights from a uniform distribution.\n","    * The uniform distribution $U(-a, a)$ has variance $\\frac{a^2}{3}$.\n","\n","$$U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right).$$"]},{"cell_type":"markdown","metadata":{"id":"RqtGwbx-W02y"},"source":["## He Initialization\n","\n","* Xavier initialization assumes non-existence of nonlinearities.\n","    * This is easily violated in neural networks.\n","* He initialization takes that into account.\n","* He initialization initializes weights from a Gaussian with zero mean and variance\n","$\\sigma^2 = \\frac{2}{n_\\mathrm{in}}$.\n","* PyTorch implementation: `torch.nn.init.kaiming_normal_`\n"]},{"cell_type":"markdown","metadata":{"id":"Q6xLUP2nW02z"},"source":["# Regularization\n","\n","* Regularisation is a standard technique to reduce overfitting in Machine Learning. Regularisation techniques for NNs include:\n","    * Early Stopping \n","    * L2 regularization (weight decay)\n","    * Dropout\n","    * Data augmentation\n","    * More exotic ones (e.g. mix-up)\n"]},{"cell_type":"markdown","metadata":{"id":"i_g2LEwliGHq"},"source":["## Early Stopping\n","\n","* Simply stop training when accuracy starts dropping on a seperate validation set"]},{"cell_type":"markdown","metadata":{"id":"eJ5zOd9ZW02z"},"source":["## Weight decay (L2 regularization)\n","\n","* **Weight Decay** (commonly called $L_2$ regularization), might be the most widely-used technique for regularizing  Machine Learning models. \n","* WD adds $\\|\\mathbf{w}\\|^2$ as an additional term to loss function\n","$$L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2,$$\n","    * For $\\lambda = 0$, we recover the original loss function $L(\\mathbf{w}, b)$.\n","    * For $\\lambda > 0$, we put a penalty on the magnitude of $\\| \\mathbf{w} \\|$.\n","* The $L_2$ norm places a penalty on large components of the weight vector.\n","    * This biases our learning algorithm towards models that distribute the weights evenly across features.\n","    * In practice, this makes them more robust to measurement error in a single variable.\n","* Other norms are possible, e.g. the $L_1$ norm.\n","    * $L_1$ penalty lead to models that most weights are 0.\n","    * This can be used for *feature selection*, which may be desirable for some applications."]},{"cell_type":"markdown","metadata":{"id":"XZL3GdbpW02z"},"source":["## Weight decay Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puixTQi-W020"},"outputs":[],"source":["# Scratch implementation\n","# We need to iterate through all params\n","def l2_penalty(w):\n","    return torch.sum(w.pow(2)) / 2\n","\n","model = torch.nn.Linear(10,10, bias=False)\n","reg_loss = 0\n","for param in model.parameters():\n","    reg_loss += l2_penalty(param) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL1B2nMhW020"},"outputs":[],"source":["# Pytorch's implementation\n","wd, lr = 0.0005, 0.1\n","model = torch.nn.Linear(10,10)\n","optimizer = torch.optim.SGD(model.parameters(), weight_decay=wd, lr=lr)"]},{"cell_type":"markdown","metadata":{"id":"UoGNpeu7W021"},"source":["## Droupout\n","\n","\n","* Neural network overfitting is characterized by a state in which each layer relies on a specifc pattern of activations in the previous layer,\n","    * This is called *co-adaptation*.\n","* Dropout is a method to break this co-adaptation. \n","* During training neurons are randomly dropped out: with *dropout probability* $p$, each intermediate activation $h$ is replaced by a random variable $h'$ as follows:\n","\n","$$\n","\\begin{aligned}\n","h' =\n","\\begin{cases}\n","    0 & \\text{ with probability } p \\\\\n","    \\frac{h}{1-p} & \\text{ otherwise}\n","\\end{cases}\n","\\end{aligned}\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qgeXBF_ZW021"},"source":["# Droupout\n","\n","* In the example below $h_2$ and $h_5$ are dropped.\n","* The calculation of the outputs no longer depends on $h_2$ or $h_5$ and their respective gradient also vanishes when performing backpropagation.\n","* In this way, the calculation of the output layer cannot be overly dependent on any one element of $h_1, \\ldots, h_5$.\n","\n","<!-- ![MLP before and after dropout.](img/dropout2.svg)  -->\n","\n","![MLP before and after dropout.](https://drive.google.com/uc?export=view&id=1h-FSwbs1KHzsW_npRWzINT8yjnCx0DG0)  \n","\n","* Typically, we disable dropout at test time.\n","    * Given a trained model and a new example, we do not drop out any nodes.\n","* Some researchers use dropout at test time as a heuristic for estimating the *uncertainty* of neural network predictions.\n","    * If the predictions agree across many different dropout masks, then we might say that the network is more confident."]},{"cell_type":"markdown","metadata":{"id":"hibJBdN9W022"},"source":["# Dropout Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpKkhL0eW022"},"outputs":[],"source":["# implementation from scratch\n","def dropout_layer(x, dropout): #x is feature tensor\n","    assert 0 <= dropout <= 1\n","    # In this case, all elements are dropped out\n","    if dropout == 1:\n","        return torch.zeros_like(x)\n","    # In this case, all elements are kept\n","    if dropout == 0:\n","        return x\n","    mask = (torch.Tensor(x.shape).uniform_(0, 1) > dropout).float()\n","    return mask * x / (1.0 - dropout)"]},{"cell_type":"markdown","metadata":{"id":"uqbW1OvLW022"},"source":["* PyTorch provides `torch.nn.Dropout(p)`\n","    * At test time remember to call `model.eval()`\n","* Dropout layer can be used after the ReLU non-linearity in each layer"]},{"cell_type":"markdown","metadata":{"id":"pf_CsBWXW023"},"source":["## Data Augmentation\n","\n","* In general we will train our network for a large number of epochs. \n","    * Each epoch we will see a training example once. \n","* From epoch to epoch what we can do is not to use exactly the same example but apply some sort of noise to it so that it looks a bit different. \n","* This is to some extent equivalent to using more data during training\n","* For images we can apply several types of noise including scaling, rotation, colour jittering\n","* An example of geometric augmentation:\n","\n","<!-- img src=\"img/augmentation.png\" alt=\"drawing\" width=\"400\"/>  -->\n","\n"," <img src=\"https://drive.google.com/uc?export=view&id=1jWn2L2gkBPts0hVMGdUJHmbcZySqLEkR\" alt=\"drawing\" width=\"400\"/> \n","\n"]},{"cell_type":"markdown","metadata":{"id":"RxpuxMU_W024"},"source":["# Learning Rate \n","\n","* Adjusting the learning rate is imporant to find good solutions. \n","* For simplicity we will consider gradient descent in one dimension.\n","* Example: consider the objective function $f(x)=x^2$. \n","* We use $x=10$ as the initial value and assume $\\eta=0.2$. \n","* Using GD to update $x$ for 10 times we see that, eventually, the value of $x$ approaches the optimal solution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlGgkvv1W025","scrolled":true},"outputs":[],"source":["f = lambda x: x**2  # Objective function\n","gradf = lambda x: 2 * x  # Its derivative\n","\n","def gd(eta):\n","    x = 10.0\n","    results = [x]\n","    for i in range(10):\n","        x -= eta * gradf(x)\n","        results.append(float(x))\n","    print('epoch 10, x:', x)\n","    return results\n","\n","res = gd(0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfog5W7hW026","scrolled":false},"outputs":[],"source":["def show_trace(res):\n","    n = max(abs(min(res)), abs(max(res)))\n","    f_line = torch.arange(-n, n, 0.01)\n","    mu.set_figsize()\n","    mu.plot([f_line, res], [[f(x) for x in f_line], [f(x) for x in res]],\n","             'x', 'f(x)', fmts=['-', '-o'])\n","\n","show_trace(res)"]},{"cell_type":"markdown","metadata":{"id":"9kRrLfwEiGHv"},"source":["\n","* We should try to use a high learning rate. \n","* This leads to both good solutions and faster convergence.\n","* If the learning rate is too high we might have unecessary oscillations close to the minumum.\n","    * Hence, we may want to reduce it after a number of epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ZbXzJn3siGHv"},"outputs":[],"source":["res = gd(0.9) \n","show_trace(res)"]},{"cell_type":"markdown","metadata":{"id":"i2pM090EiGHv"},"source":["* Too high a learning rate results in divergence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCUScEJbiGHw"},"outputs":[],"source":["res = gd(1.0)\n","show_trace(res)"]},{"cell_type":"markdown","metadata":{"id":"rl0YIXDPW027"},"source":["# Summary\n","\n","\n","* Random initialization is key to ensure that symmetry is broken before optimization.\n","* Xavier initialization suggests that, for each layer, variance of any output is not affected by the number of inputs, and variance of any gradient is not affected by the number of outputs."]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}