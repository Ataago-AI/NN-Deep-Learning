{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"LR_Concise.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Dz7XBNODEa-H","executionInfo":{"status":"ok","timestamp":1610915907655,"user_tz":0,"elapsed":4231,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["%matplotlib inline\n","#import my_utils as mu\n","import torch\n","from torch.utils import data\n","from torch import nn\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"b129ZxUYEa-O"},"source":["# Concise Implementation of Linear Regression\n","\n","* Deep Learning algorithms are implemented using frameworks like PyTorch\n","* So far from PyTorch we relied only on (i) tensors for data storage and linear algebra; and (ii) auto differentiation for calculating gradients.\n","* In practice, because data iterators, loss functions, optimizers, and neural network layers are provided as well.\n","* Here, we will see how to implement the linear regression model  concisely by using the high-level API of PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"ypLcYVK4Ea-O"},"source":["# Generating the Dataset"]},{"cell_type":"code","metadata":{"origin_pos":4,"tab":["pytorch"],"id":"yIqTr3LqEa-P","executionInfo":{"status":"ok","timestamp":1610915912675,"user_tz":0,"elapsed":928,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["# Generating the Dataset\n","def synthetic_data(w, b, num_examples):  \n","    \"\"\"Generate y = Xw + b + noise.\"\"\"\n","    X = torch.normal(0, 1, (num_examples, len(w)))\n","    y = torch.mm(X, w) + b\n","    y += torch.normal(0, 0.01, y.size())\n","    return X, y.reshape((-1, 1))\n","\n","true_w = torch.tensor([[2], [-3.4]])\n","true_b = 4.2\n","features, labels = synthetic_data(true_w, true_b, 1000)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"rekqORZsEa-P"},"source":["# Reading the Dataset\n","* We need 2 things: (a) A way to access the dataset and (b) A way to iterate through it.\n","* For (a), PyTorch has an abstract Dataset class. A Dataset can be anything that has a `__len__ `function (called by Python’s standard `len` function) and a `__getitem__` function as a way of indexing into it.\n","* PyTorch’s TensorDataset is a Dataset wrapper for tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. \n","    * In general we will have to implement our own Dataset class, extending `torch.utils.data.Dataset`\n","* For (b), we use `torch.utils.data.DataLoader`. In addition to iterate through the dataset, this also provides built-in functionality for: 1. Batching the data, 2. Shuffling the data and 3. **Load the data in parallel using multiprocessing workers.**"]},{"cell_type":"code","metadata":{"origin_pos":7,"scrolled":true,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"FFfiz-8xEa-Q","executionInfo":{"status":"ok","timestamp":1610915916141,"user_tz":0,"elapsed":800,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"db06c610-a01b-4238-fde6-cae2ef8a379f"},"source":["dataset = data.TensorDataset(features, labels) #TensorDataset object\n","print(dataset[0]) #First example in our dataset\n","batch_size = 10\n","data_iter =  data.DataLoader(dataset, batch_size, shuffle=True) #DataLoader object"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(tensor([1.3314, 0.0222]), tensor([6.7981]))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":10,"id":"HTzxlI8WEa-R"},"source":["# Reading the Dataset\n","* `data_iter` is iterable object: we can use `iter` to construct a Python iterator and use `next` to obtain the first item from it.\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"HdSp5q8yEa-R","executionInfo":{"status":"ok","timestamp":1610915919615,"user_tz":0,"elapsed":941,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"c84b4204-a40d-47af-ad91-4e09d5f58ab5"},"source":["next(iter(data_iter))"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[ 1.1954, -0.1060],\n","         [-0.5514,  0.4698],\n","         [-2.6551, -0.3891],\n","         [-0.5129, -1.2192],\n","         [ 1.5999, -1.0181],\n","         [ 0.8643, -0.4946],\n","         [-0.5823,  1.4793],\n","         [-0.0861,  0.0900],\n","         [-2.0686,  0.4720],\n","         [-0.1090, -0.4890]]), tensor([[ 6.9446],\n","         [ 1.5093],\n","         [ 0.2144],\n","         [ 7.3189],\n","         [10.8652],\n","         [ 7.6082],\n","         [-1.9896],\n","         [ 3.7246],\n","         [-1.5522],\n","         [ 5.6601]])]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"XEiSjVwVEa-S"},"source":["# Defining the Model\n","\n","* When we implemented linear regression from scratch, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations.\n","* But once models get more complex, and for standard operations, we can use a framework's predefined layers\n","* For Linear Regression we need a single Linear (or Fully-Connected) layer: each of its inputs is connected to each of its outputs by means of a matrix-vector multiplication.\n","* In PyTorch, the FC layer is defined in the `Linear` class. Note that we passed two arguments into `nn.Linear`. \n"]},{"cell_type":"code","metadata":{"origin_pos":17,"tab":["pytorch"],"id":"5cSU9gBAEa-S","executionInfo":{"status":"ok","timestamp":1610915922607,"user_tz":0,"elapsed":821,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["num_of_inp, num_of_out = 2, 1 # input, output feature dimension\n","net = nn.Linear(num_of_inp, num_of_out)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":19,"id":"THI1NMumEa-S"},"source":["## Initializing Model Parameters\n","\n","* Deep learning frameworks have a predefined way to initialize the parameters.\n","* We can access the parameters directly to specify the initial values: \n","    * We use the `weight.data` and `bias.data` methods to access the parameters. \n","    * Finally use the inplace methods `normal_` and `fill_` to overwrite parameter values."]},{"cell_type":"code","metadata":{"origin_pos":24,"tab":["pytorch"],"id":"n4kdhcFbEa-T","executionInfo":{"status":"ok","timestamp":1610915925305,"user_tz":0,"elapsed":783,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["net.weight.data.normal_(0, 0.01); # each weight sampled from a Gaussian with mean 0 and std 0.01.\n","net.bias.data.fill_(0); # bias initialized to 0."],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":29,"id":"uVx3xZgvEa-U"},"source":["# Defining the Loss Function\n","* The `MSELoss` class computes the mean squared error, also known as squared $L_2$ norm.\n","* By default it returns the average loss over examples."]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"id":"Hb6RDaH-Ea-U","executionInfo":{"status":"ok","timestamp":1610915929266,"user_tz":0,"elapsed":1062,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["loss = nn.MSELoss()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"oY5C13wpEa-V"},"source":["# Defining the Optimization Algorithm\n","\n","* Minibatch SGD is a standard tool for optimizing neural networks and thus PyTorch supports it \n","    * A number of variations of this algorithm can be found in the `optim` module.\n","* To initialize an `SGD` optimizer we need:\n","    * The parameters to optimize (obtainable from our net via `net.parameters()`), \n","    * A dictionary of hyperparameters required.\n","        * For now we just require to set the `lr`"]},{"cell_type":"code","metadata":{"origin_pos":41,"tab":["pytorch"],"id":"zrgVW6MhEa-V","executionInfo":{"status":"ok","timestamp":1610915931929,"user_tz":0,"elapsed":875,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}}},"source":["optimizer = torch.optim.SGD(net.parameters(), lr=0.03)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":43,"id":"VWrjKTnSEa-V"},"source":["# Training\n","\n","* The training loop is similar to the one from implementing everything from scratch:\n","    * For each epoch, we will make a complete pass over the dataset, iteratively grabbing one minibatch of inputs and ground-truth labels.\n","    * For each minibatch:\n","        * Generate predictions by calling `net(X)` and calculate the loss `l` (the forward pass).\n","        * Calculate gradients by running the backpropagation.\n","        * Update the model parameters by invoking our optimizer.\n","* Compute the loss after each epoch and print it to monitor progress.\n"]},{"cell_type":"code","metadata":{"origin_pos":45,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"wDd9wQBBEa-W","executionInfo":{"status":"ok","timestamp":1610915934441,"user_tz":0,"elapsed":803,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"378879ee-1e8d-41b4-88aa-9824fe37c1fc"},"source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        y_hat = net(X)\n","        #print(y.size(), y_hat.size())\n","        l = loss(y_hat, y)\n","        optimizer.zero_grad()\n","        l.backward()\n","        optimizer.step()\n","    l = loss(net(features), labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["epoch 1, loss 0.000403\n","epoch 2, loss 0.000104\n","epoch 3, loss 0.000104\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":47,"id":"WQlzvMCREa-W"},"source":["# Evaluation\n","\n","* Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset.\n"]},{"cell_type":"code","metadata":{"origin_pos":49,"scrolled":true,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"f_I7E7KyEa-X","executionInfo":{"status":"ok","timestamp":1610915936740,"user_tz":0,"elapsed":902,"user":{"displayName":"Georgios Tzimiropoulos","photoUrl":"","userId":"03131831684678803743"}},"outputId":"980a52d1-4d95-4c6c-c931-dd4fd421e9d5"},"source":["w = net.weight.data\n","print('error in estimating w:', true_w - w.reshape(true_w.shape))\n","b = net.bias.data\n","print('error in estimating b:', true_b - b)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["error in estimating w: tensor([[-0.0006],\n","        [-0.0005]])\n","error in estimating b: tensor([7.0572e-05])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":53,"tab":["pytorch"],"id":"OVMsRIJiEa-X"},"source":["# Summary\n","\n","* Using PyTorch's high-level API, we can implement models much more concisely.\n","* In PyTorch, the `data` module provides tools for data processing, the `nn` module defines a large number of neural network layers and common loss functions.\n","* Initialize the parameters by replacing their values with methods ending with `_`.\n"]}]}