{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EzCos2IunsdM"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elyRUwxtnqYK"},"outputs":[],"source":["%matplotlib inline\n","import my_utils as mu\n","import math\n","import torch\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"eAFKZXzwnqYY"},"source":["# Desired Features of RNNs\n","\n","  \n","* Have a mechanism for storing vital early information in a *memory cell*. \n","    * An early observation is highly significant for predicting all future observations.\n","    * Without such a mechanism, we will have to assign a very large gradient to this observation\n","* Handle situations where some symbols carry no pertinent information and have a mechanism for *skipping such symbols* in the latent state representation. \n","  * E.g. when parsing a web page there might be auxiliary HTML code that is irrelevant for the purpose of assessing the sentiment conveyed on the page.  \n","* Have a mechanism of *resetting* our internal state representation.\n","    * When there is a logical break between parts of a sequence, e.g. a transition between chapters in a book."]},{"cell_type":"markdown","metadata":{"id":"ysiE0nqunqYa"},"source":["# LSTMs and GRUs\n","\n","* The Long Short Term Memory (LSTM) is an Advanced RNN which can handle all of the above requirements.\n","* The Gated Recurrent Unit (GRU) is a simplied version of LSTM.\n","    * Due to its simplicity, we will start with the GRU.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"44VvAl__nqYb"},"source":["# Gated Recurrent Units (GRU)\n","\n","\n","## Gating the Hidden State\n","\n","* The key distinction between regular RNNs and GRUs is that the latter support gating of the hidden state. \n","    * This means that we have dedicated mechanisms for when a hidden state should be updated and also when it should be reset. \n","    \n","* These mechanisms are learned: \n","     * E.g., if the first symbol is of great importance, it will learn not to update the hidden state after the first observation. \n","     * Likewise, it will learn to skip irrelevant temporary observations. \n","     * Last, it can learn to reset the latent state whenever needed. "]},{"cell_type":"markdown","metadata":{"id":"T-8mtitsnqYc"},"source":["\n","# Reset Gates and Update Gates\n","\n","* The first thing we need to introduce are reset and update gates.  \n","* A reset gate would allow us to control how much of the previous state we might still want to remember.\n","* An update gate would allow us to control how much of the new state is just a copy of the old state.\n","* We engineer them to be vectors with entries in $(0, 1)$."]},{"cell_type":"markdown","metadata":{"id":"e3KxqaBqnqYd"},"source":["* The following figure shows how the reset and update gates are computed a each time step: \n","  * They are functions of $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$. \n","  * The function output is given by a fully connected layer with a sigmoid as its activation function.\n","\n","<!-- ![ Reset and update gate in a GRU. ](img/gru_1.svg) -->\n","\n","![Reset and update gate in a GRU. ](https://drive.google.com/uc?export=view&id=1g_auUeCt5EQXCRWUTpdBuE0V_EyqtmiY)   \n","\n"]},{"cell_type":"markdown","metadata":{"id":"E3mLEkRLnqYd"},"source":["\n","* Assume, for a given time step $t$, the minibatch input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ (number of examples: $n$, number of inputs: $d$) and the hidden state of the last time step is $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$ (number of hidden states: $h$). \n","\n","* Then, the reset gate $\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$ and update gate $\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$ are computed as follows:\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n","\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z).\n","\\end{aligned}\n","$$\n","\n","* $\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$ are\n","biases. \n","* A sigmoid function is used to transform input values to the interval $(0, 1)$."]},{"cell_type":"markdown","metadata":{"id":"ykQ7O37PnqYf","origin_pos":0},"source":["# Reset Gates in Action\n","\n","* In a conventional RNN, we would have an hidden state update of the form\n","\n","$$\\mathbf{H}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h).$$\n","\n","\n","* If we want to reduce the influence of the previous states we can multiply $\\mathbf{H}_{t-1}$ with $\\mathbf{R}_t$ elementwise. \n","    * For all entries of $\\mathbf{R}_t$ that are close to $1$, we recover a conventional RNN. \n","    * For all entries of $\\mathbf{R}_t$ that are close to $0$, the pre-existing hidden state is reset to defaults. \n","        \n","* This leads to the following *candidate hidden state*:\n","\n","$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h).$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uzBE6a_unqYh"},"source":["* The following figure illustrates the computational flow after applying the reset gate. \n","  * The symbol $\\odot$ indicates elementwise multiplication between tensors.\n","\n","<!-- ![ Candidate hidden state computation in a GRU. The multiplication is carried out elementwise. ](img/gru_2.svg)  -->\n","\n"," ![ Candidate hidden state computation in a GRU. The multiplication is carried out elementwise. ](https://drive.google.com/uc?export=view&id=1LE9q5hFWtIKuySB_fKPkiBepoDyUA7CR) "]},{"cell_type":"markdown","metadata":{"id":"2zv0pYBPnqYi"},"source":["# Update Gates in Action\n","\n","* The update gate $\\mathbf{Z}_t$, determines the extent to which the new state $\\mathbf{H}_t$ is just the old state $\\mathbf{H}_{t-1}$ and by how much the new candidate state $\\tilde{\\mathbf{H}}_t$ is used. \n","\n","* This leads to the final update equation for the GRU:\n","\n","$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n","\n","* Whenever $\\mathbf{Z}_t$ is close to $1$, we simply retain the old state. \n","    * In this case $\\mathbf{X}_t$ is essentially ignored, effectively skipping time step $t$ in the dependency chain. \n","* Whenever $\\mathbf{Z}_t$ is close to $0$, the new latent state $\\mathbf{H}_t$ becomes the candidate latent state $\\tilde{\\mathbf{H}}_t$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-n0sfUK0nqYj"},"source":["* Put everything together the GRU is shown in the following figure:\n","\n","<!-- ![ Hidden state computation in a GRU. As before, the multiplication is carried out elementwise. ](img/gru_3.svg) -->\n","\n","![ Hidden state computation in a GRU. As before, the multiplication is carried out elementwise. ](https://drive.google.com/uc?export=view&id=1_ZTvSMWMGvQruQent0FRKIgJ_7m9YqPE)   \n","\n","* In summary, GRUs have the following two distinguishing features:\n","    * Reset gates help capture short-term dependencies in time series.\n","    * Update gates help capture long-term dependencies in time series.\n","\n","* These designs help with:\n","    * Cope with the vanishing gradient problem in RNNs  \n","    * Better capture dependencies for time series with large time step distances. \n"]},{"cell_type":"markdown","metadata":{"id":"x5nk5B8_nqYl"},"source":["# Concise Implementation of GRU "]},{"cell_type":"markdown","metadata":{"id":"VxitNwl5nqYl"},"source":["# Loading the Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTumqFCznqYm","origin_pos":2,"tab":["pytorch"]},"outputs":[],"source":["batch_size, num_steps = 32, 35\n","train_iter, vocab = mu.load_data_time_machine(batch_size, num_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GyWCt4anqYu"},"outputs":[],"source":["# RNNModel class contains a complete RNN model.\n","# rnn_layer only contains the hidden recurrent layers, we need to create a separate output layer.\n","class RNNModel(nn.Module):\n","    \"\"\"The RNN model.\"\"\"\n","    def __init__(self, rnn_layer, vocab_size):\n","        super(RNNModel, self).__init__()\n","        self.rnn = rnn_layer\n","        self.vocab_size = vocab_size\n","        self.num_hiddens = self.rnn.hidden_size\n","        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n","        \n","    def forward(self, inputs, state):\n","        X = F.one_hot(inputs.T.long(), self.vocab_size)\n","        X = X.to(torch.float32)\n","        Y, state = self.rnn(X, state)\n","        #print(X.size()) # 35x32x28\n","        #print(Y.size()) # 35x32x256\n","        #print(state.size()) # 32x256\n","        Y1 = Y.reshape((-1, Y.shape[-1]))\n","        #print(Y1.size()) # 1120x256\n","        out = self.linear(Y1) \n","        #print(out.size()) # 1120x28\n","        return out, state\n","\n","    def begin_state(self, batch_size=1):\n","        state = torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens))\n","        return state "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyzZIIIAnqYv"},"outputs":[],"source":["def train_epoch_ch8(model, train_iter, loss, optimizer, use_random_iter):\n","    \"\"\"Train a model for one epoch \"\"\"\n","    state = None\n","    metric = mu.Accumulator(2)  # Sum of training loss, no. of tokens\n","    for X, Y in train_iter:\n","        # Initialize `state` when first iteration or using random sampling\n","        if state is None or use_random_iter:\n","            state = model.begin_state(batch_size=X.shape[0])\n","        else:\n","            if isinstance(model, nn.Module) and not isinstance(state, tuple):\n","            # `state` is a tensor for `nn.GRU`\n","                state.detach_()\n","        #print(X.size(), Y.size(), state.size()) # 32x35, 32x35, 32x256\n","        y = Y.T.reshape(-1) \n","        #print(y.size()) # 35x32 -> 1120 \n","        y_hat, state = model(X, state)\n","        #print(y_hat.size()) # 1120x28 \n","        l = loss(y_hat, y.long())\n","        optimizer.zero_grad()\n","        l.backward()\n","        mu.grad_clipping(model, 1)\n","        optimizer.step()\n","    \n","        metric.add(l * mu.size(y), mu.size(y))\n","    return math.exp(metric[0] / metric[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2NjzfNenqYx"},"outputs":[],"source":["def train_ch8(model, train_iter, vocab, lr, num_epochs, use_random_iter=False):\n","    \"\"\"Train a model for num_epochs\"\"\"\n","    animator = mu.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[1, num_epochs])\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr)\n","    # Train and predict\n","    for epoch in range(num_epochs):\n","        ppl = train_epoch_ch8(model, train_iter, loss, optimizer, use_random_iter)\n","        if epoch % 10 == 0:\n","            print(predict_ch8('time traveller', 50, model, vocab))\n","            animator.add(epoch + 1, [ppl])\n","    print(f'perplexity {ppl:.1f}')\n","    print(predict_ch8('time traveller', 50, model, vocab))\n","    print(predict_ch8('traveller', 50, model, vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPMdQp6LbaO4"},"outputs":[],"source":["def predict_ch8(prefix, num_preds, model, vocab):  \n","    \"\"\"Generate new characters following the `prefix`.\"\"\"\n","    state = model.begin_state(batch_size=1)\n","    outputs = [vocab[prefix[0]]]\n","    get_input = lambda: mu.reshape(torch.tensor([outputs[-1]]), (1, 1))\n","    for y in prefix[1:]:  # Warm-up period\n","        _, state = model(get_input(), state)\n","        outputs.append(vocab[y])\n","    for _ in range(num_preds):  # Predict `num_preds` steps\n","        y, state = model(get_input(), state)\n","        outputs.append(int(y.argmax(dim=1).reshape(1)))\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])"]},{"cell_type":"markdown","metadata":{"id":"xuR22KhHnqYx"},"source":["# Training and Prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72GfrV6knqYx","origin_pos":16,"tab":["pytorch"]},"outputs":[],"source":["vocab_size, num_hiddens = len(vocab), 256\n","num_epochs, lr = 500, 1\n","num_inputs = vocab_size\n","gru_layer = nn.GRU(num_inputs, num_hiddens)\n","model = RNNModel(gru_layer, len(vocab))\n","train_ch8(model, train_iter, vocab, lr, num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"d1SZRxaknqYy","origin_pos":17},"source":["# Summary\n","\n","* Gated recurrent neural networks are better at capturing dependencies for time series with large time step distances.\n","* Reset gates help capture short-term dependencies in time series.\n","* Update gates help capture long-term dependencies in time series.\n","* GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They can ignore sequences as needed.\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}