{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"H30pAhUpT_z1"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahwDTglET-g2"},"outputs":[],"source":["%matplotlib inline\n","import my_utils as mu\n","import math\n","import torch\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"DsfAuR80T-g9","origin_pos":0},"source":["# Concise Implementation of RNNs\n","\n","* Here we will see how to implement the same language model more efficiently using functions provided by Pytorch.\n"]},{"cell_type":"markdown","metadata":{"id":"48MtYVDGT-g-"},"source":["# Loading the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baWnKpD1T-g-","origin_pos":2,"tab":["pytorch"]},"outputs":[],"source":["batch_size, num_steps = 32, 35\n","train_iter, vocab = mu.load_data_time_machine(batch_size, num_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JySRkl63YKq_"},"outputs":[],"source":["print(list(vocab.token_to_idx.items())[:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhIxl8JDYKrB"},"outputs":[],"source":["train_iterator = iter(train_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqrJYf7oYKrC"},"outputs":[],"source":["batch_1 = next(train_iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDNDsUn4YKrD"},"outputs":[],"source":["sample_1 = batch_1[0][1, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDq3tAz_YKrE"},"outputs":[],"source":["sample_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOAGYa6rYKrG"},"outputs":[],"source":["sample_token = [vocab.idx_to_token[idx] for idx in sample_1]\n","print(sample_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30OOGIIfYKrH"},"outputs":[],"source":["sample_1_y = batch_1[1][1, :]\n","print(sample_1_y)"]},{"cell_type":"markdown","metadata":{"id":"K-xkGYzBT-g_","origin_pos":3},"source":["# Defining the Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LijamBCIT-g_","origin_pos":5,"tab":["pytorch"]},"outputs":[],"source":["# An RNN with a single hidden layer and 256 hidden units.\n","num_hiddens = 256 \n","rnn_layer = nn.RNN(input_size = len(vocab), hidden_size= num_hiddens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egvcsUlkT-g_","origin_pos":9,"tab":["pytorch"]},"outputs":[],"source":["# A tensor is used to initialize the hidden state\n","state = torch.zeros((1, batch_size, num_hiddens))\n","state.size() #number of hidden layers x batch size x number of hidden units"]},{"cell_type":"markdown","metadata":{"id":"q-rPRDxfT-hB","origin_pos":10},"source":["* Given the previous hidden state and an input, we can compute the updated hidden state.\n","* The \"output\" (`Y`) of `rnn_layer` refers to the hidden state at *each* time step, not the output of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fN05LYkiT-hC","origin_pos":13,"tab":["pytorch"]},"outputs":[],"source":["X = torch.rand(num_steps, batch_size, len(vocab))\n","X.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTr18FQPYKrO"},"outputs":[],"source":["Y, state_new = rnn_layer(X, state)\n","Y.size(), state_new.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6gBFvm_T-hD","origin_pos":16,"tab":["pytorch"]},"outputs":[],"source":["# RNNModel class contains a complete RNN model.\n","# rnn_layer only contains the hidden recurrent layers, we need to create a separate output layer.\n","class RNNModel(nn.Module):\n","    \"\"\"The RNN model.\"\"\"\n","    def __init__(self, rnn_layer, vocab_size):\n","        super(RNNModel, self).__init__()\n","        self.rnn = rnn_layer\n","        self.vocab_size = vocab_size\n","        self.num_hiddens = self.rnn.hidden_size\n","        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n","        \n","    def forward(self, inputs, state):\n","        X = F.one_hot(inputs.T.long(), self.vocab_size)\n","        X = X.to(torch.float32)\n","        Y, state = self.rnn(X, state)\n","        #print(X.size()) # 35x32x28\n","        #print(Y.size()) # 35x32x256\n","        #print(state.size()) # 32x256\n","        Y1 = Y.reshape((-1, Y.shape[-1]))\n","        #print(Y1.size()) # 1120x256\n","        out = self.linear(Y1) \n","        #print(out.size()) # 1120x28\n","        return out, state\n","\n","    def begin_state(self, batch_size=1):\n","        state = torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens))\n","        return state "]},{"cell_type":"markdown","metadata":{"id":"pJdbW8lYT-hD","origin_pos":17},"source":["# Training and Predicting\n","\n","* Before training the model, let us make a prediction with the a model that has random weights.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfzkHNOVYKrT"},"outputs":[],"source":["def predict_ch8(prefix, num_preds, model, vocab):  \n","    \"\"\"Generate new characters following the `prefix`.\"\"\"\n","    state = model.begin_state(batch_size=1)\n","    outputs = [vocab[prefix[0]]]\n","    get_input = lambda: mu.reshape(torch.tensor([outputs[-1]]), (1, 1))\n","    for y in prefix[1:]:  # Warm-up period\n","        _, state = model(get_input(), state)\n","        outputs.append(vocab[y])\n","    for _ in range(num_preds):  # Predict `num_preds` steps\n","        y, state = model(get_input(), state)\n","        outputs.append(int(y.argmax(dim=1).reshape(1)))\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbND0unCT-hD","origin_pos":19,"scrolled":true,"tab":["pytorch"]},"outputs":[],"source":["model = RNNModel(rnn_layer, vocab_size=len(vocab))\n","predict_ch8('time traveller', 10, model, vocab)"]},{"cell_type":"markdown","metadata":{"id":"wWIKnRhST-hE","origin_pos":20},"source":["* Training with `train_ch8` with the same hyperparameters as before\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQWiIml2T-hE"},"outputs":[],"source":["def train_epoch_ch8(model, train_iter, loss, optimizer, use_random_iter):\n","    \"\"\"Train a model for one epoch \"\"\"\n","    state = None\n","    metric = mu.Accumulator(2)  # Sum of training loss, no. of tokens\n","    for X, Y in train_iter:\n","        # Initialize `state` when first iteration or using random sampling\n","        if state is None or use_random_iter:\n","            state = model.begin_state(batch_size=X.shape[0])\n","        #print(X.size(), Y.size(), state.size()) # 32x35, 32x35, 32x256\n","        y = Y.T.reshape(-1) \n","        #print(y.size()) # 35x32 -> 1120 \n","        y_hat, state = model(X, state)\n","        #print(y_hat.size()) # 1120x28 \n","        l = loss(y_hat, y.long())\n","        optimizer.zero_grad()\n","        l.backward()\n","        mu.grad_clipping(model, 1)\n","        optimizer.step()\n","    \n","        metric.add(l * mu.size(y), mu.size(y))\n","    return math.exp(metric[0] / metric[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDQG9nkOT-hF"},"outputs":[],"source":["def train_ch8(model, train_iter, vocab, lr, num_epochs, use_random_iter=False):\n","    \"\"\"Train a model for num_epochs\"\"\"\n","    animator = mu.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[1, num_epochs])\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr)\n","    # Train and predict\n","    for epoch in range(num_epochs):\n","        ppl = train_epoch_ch8(model, train_iter, loss, optimizer, use_random_iter)\n","        if epoch % 10 == 0:\n","            print(predict_ch8('time traveller', 50, model, vocab))\n","            animator.add(epoch + 1, [ppl])\n","    print(f'perplexity {ppl:.1f}')\n","    print(predict_ch8('time traveller', 50, model, vocab))\n","    print(predict_ch8('traveller', 50, model, vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atIQE4n8T-hF","origin_pos":21,"tab":["pytorch"]},"outputs":[],"source":["num_epochs, lr = 500, 0.1\n","train_ch8(model, train_iter, vocab, lr, num_epochs, device)"]},{"cell_type":"markdown","metadata":{"id":"ebp_f2d6T-hG","origin_pos":22},"source":["# Summary\n","\n","* PyTorch provides an implementation of the RNN layer.\n","* The RNN layer of PyTorch returns an output and an updated hidden state, where the output does not involve output layer computation.\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}