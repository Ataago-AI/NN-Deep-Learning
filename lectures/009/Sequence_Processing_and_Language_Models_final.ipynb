{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VrtlT1Ydd31q"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXFx6qzYdwYk"},"outputs":[],"source":["import torch\n","import collections\n","import my_utils as mu\n","import re\n","import random"]},{"cell_type":"markdown","metadata":{"id":"9HrPd9lBdwYx"},"source":["# Recurrent Neural Networks\n","\n","* So far we encountered two types of data: tabular data and image data.\n","* However, there are other types of data following a sequential order:\n","    * words in sentences\n","    * image frames in a video\n","    * the audio signal in a conversation\n","    * the browsing behavior on a website\n","    \n","* It is reasonable to assume that specialized models for such data will do better at describing them.\n","\n","* In short, while CNNs can efficiently process spatial information, *recurrent neural networks* (RNNs) are designed to better handle sequential information.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7LZYNfEvdwYy"},"source":["# Text Preprocessing\n","\n","* Text is one of the most popular examples of sequence data.\n","* For example, an article can be simply viewed as a sequence of words, or even a sequence of characters.\n","* Common preprocessing steps for text include:\n","    1. Load text as strings into memory.\n","    1. Split strings into tokens (e.g., words and characters).\n","    1. Build a table of vocabulary to map the split tokens to numerical indices.\n","    1. Convert text into sequences of numerical indices so they can be manipulated by models easily."]},{"cell_type":"markdown","metadata":{"id":"ERVk8gEgdwYz"},"source":["# Reading the Dataset\n","\n","* We will use the text from H. G. Wells' [*The Time Machine*](http://www.gutenberg.org/ebooks/35).\n","* A fairly small corpus of just over 30000 words\n","    * More realistic document collections contain many billions of words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHux5RCqdwY0"},"outputs":[],"source":["# The following function reads the dataset into a list of text lines, where each line is a string.\n","# For simplicity, punctuation and capitalization are ignored\n","mu.DATA_HUB['time_machine'] = (mu.DATA_URL + 'timemachine.txt',\n","                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n","\n","def read_time_machine():  \n","    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n","    with open(mu.download('time_machine'), 'r') as f:\n","        lines = f.readlines()\n","    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n","\n","lines = read_time_machine()\n","print(f'# text lines: {len(lines)}')\n","print(lines[0])\n","print(lines[10])"]},{"cell_type":"markdown","metadata":{"id":"S72Y1eTHdwY3"},"source":["# Tokenization\n","\n","* The following `tokenize` function takes a list (`lines`) as the input, where each list is a text sequence (e.g., a text line).\n","* Each text sequence is split into a list of tokens.\n","* A *token* is the basic unit in text.\n","* In the end, a list of token lists are returned, where each token is a string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzVA5ntMdwY4","scrolled":true},"outputs":[],"source":["def tokenize(lines, token='word'): \n","    \"\"\"Split text lines into word or character tokens.\"\"\"\n","    if token == 'word':\n","        return [line.split() for line in lines]\n","    elif token == 'char':\n","        return [list(line) for line in lines]\n","    else:\n","        print('ERROR: unknown token type: ' + token)\n","\n","tokens = tokenize(lines)\n","for i in range(11):\n","    print(tokens[i])"]},{"cell_type":"markdown","metadata":{"id":"SCqd6wwQdwY5"},"source":["# Vocabulary\n","\n","* The string type of the token is inconvenient to be used by models, which take numerical inputs.\n","* A *vocabulary* is a dictionary for mapping string tokens into numerical indices starting from 0.\n","    * First count the unique tokens in all the documents from the training set, also called a *corpus*,\n","        * Then assign a numerical index to each unique token.\n","        * Rarely appeared tokens are often removed to reduce the complexity.\n","        * Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”.\n","        * Optional: add a list of reserved tokens, such as “&lt;pad&gt;” for padding, “&lt;bos&gt;” to present the beginning for a sequence, and “&lt;eos&gt;” for the end of a sequence."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1674657458309,"user":{"displayName":"Ioanna Ntinou","userId":"17583344364006415397"},"user_tz":0},"id":"mjb_7bq-dwY6"},"outputs":[],"source":["class Vocab:  \n","    \"\"\"Vocabulary for text.\"\"\"\n","    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n","        if tokens is None:\n","            tokens = []\n","        if reserved_tokens is None:\n","            reserved_tokens = [] \n","        # Sort according to frequencies\n","        counter = count_corpus(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[0])\n","        self.token_freqs.sort(key=lambda x: x[1], reverse=True)\n","        # The index for the unknown token is 0\n","        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n","        uniq_tokens += [token for token, freq in self.token_freqs\n","                        if freq >= min_freq and token not in uniq_tokens]\n","        self.idx_to_token, self.token_to_idx = [], dict()\n","        for token in uniq_tokens:\n","            self.idx_to_token.append(token)\n","            self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token_to_idx.get(tokens, self.unk)\n","        return [self.__getitem__(token) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","def count_corpus(tokens):  \n","    \"\"\"Count token frequencies.\"\"\"\n","    # Here `tokens` is a 1D list or 2D list\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        # Flatten a list of token lists into a list of tokens\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qugO4qssdwY7"},"outputs":[],"source":["# Construct a vocabulary using the time machine dataset as the corpus. \n","vocab = Vocab(tokens)\n","# Print the first few frequent tokens with their indices.\n","print(list(vocab.token_to_idx.items())[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzZZqylOdwY8","scrolled":true},"outputs":[],"source":["# Convert each text line into a list of numerical indices.\n","for i in [0, 10]:\n","    print('words:', tokens[i])\n","    print('indices:', vocab[tokens[i]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqa_Ys-iLLjt"},"outputs":[],"source":["vocab.to_tokens([1, 3])"]},{"cell_type":"markdown","metadata":{"id":"fUVThIqEdwY9"},"source":["# Putting Everything Together\n","\n","* Using the above functions, we package everything into the `load_corpus_time_machine` function, which returns `corpus`, a list of token indices, and `vocab`, the vocabulary of the time machine corpus.\n","* The modifications we did here are:\n","   1. we tokenize text into characters, not words, to simplify the training in later sections;\n","   1. `corpus` is a single list, not a list of token lists, since each text line in the time machine dataset is not necessarily a sentence or a paragraph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x83G2uRNdwY-"},"outputs":[],"source":["def load_corpus_time_machine(max_tokens=-1): \n","    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n","    lines = read_time_machine()\n","    tokens = tokenize(lines, 'word')\n","    vocab = Vocab(tokens)\n","    # Since each text line in the time machine dataset is not necessarily a\n","    # sentence or a paragraph, flatten all the text lines into a single list\n","    corpus = [vocab[token] for line in tokens for token in line]\n","    if max_tokens > 0:\n","        corpus = corpus[:max_tokens]\n","    return corpus, vocab\n","\n","corpus, vocab = load_corpus_time_machine()\n","len(corpus), len(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIBU7K1tLLjw"},"outputs":[],"source":["type(corpus)"]},{"cell_type":"markdown","metadata":{"id":"l8qqQkW7dwY_"},"source":["# Reading Long Sequence Data\n","\n","* Since a text sequence can be arbitrarily long, we will partition it into subsequences with the same number of time steps.\n","* When training our neural network, a minibatch of such subsequences will be fed into the model.\n","* Suppose that the network processes a subsequence of $n$ time steps at a time.\n","* The figure below shows all the different ways to obtain subsequences from an original text sequence, where $n=5$ and a token at each time step corresponds to a character.\n","* We could pick any arbitrary offset that indicates the initial position.\n","* In practice we pick a random offset to partition a sequence\n","\n","<!-- ![Different offsets lead to different subsequences when splitting up text.](img/timemachine-5gram.svg) -->\n","\n","![Different offsets lead to different subsequences when splitting up text.](https://drive.google.com/uc?export=view&id=1kmt6ZARG6N4G02Iff8awphiizW0ikxL7)   \n"]},{"cell_type":"markdown","metadata":{"id":"R_y1ikiddwZA"},"source":["# Random Sampling\n","\n","* Each example is a subsequence from the original long sequence.\n","* The subsequences from two adjacent random minibatches are not necessarily adjacent in the original sequence.\n","* For language modeling, the target is to predict the next token, hence the labels are the original sequence, shifted by one token."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674657489886,"user":{"displayName":"Ioanna Ntinou","userId":"17583344364006415397"},"user_tz":0},"id":"MNOblRhYdwZB"},"outputs":[],"source":["# `num_steps` is the predefined number of time steps in each subsequence\n","def seq_data_iter_random(corpus, batch_size, num_steps):  \n","    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n","    # Start with a random offset to partition a sequence\n","    corpus = corpus[random.randint(0, num_steps):]\n","    # Subtract 1 since we need to account for labels\n","    num_subseqs = (len(corpus) - 1) // num_steps\n","    # The starting indices for subsequences of length `num_steps`\n","    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n","    # In random sampling, the subsequences from two adjacent random\n","    # minibatches during iteration are not necessarily adjacent on the\n","    # original sequence\n","    random.shuffle(initial_indices)\n","\n","    def data(pos):\n","        # Return a sequence of length `num_steps` starting from `pos`\n","        return corpus[pos: pos + num_steps]\n","\n","    num_subseqs_per_example = num_subseqs // batch_size\n","    for i in range(0, batch_size * num_subseqs_per_example, batch_size):\n","        # Here, `initial_indices` contains randomized starting indices for\n","        # subsequences\n","        initial_indices_per_batch = initial_indices[i: i + batch_size]\n","        X = [data(j) for j in initial_indices_per_batch]\n","        Y = [data(j + 1) for j in initial_indices_per_batch]\n","        yield torch.tensor(X), torch.tensor(Y)"]},{"cell_type":"markdown","metadata":{"id":"QHO-ByzxdwZC"},"source":["* Example: Generate a sequence from 0 to 34. Batch size and numbers of time steps are 2 and 5,\n","* This means that we can generate $\\lfloor (35 - 1) / 5 \\rfloor= 6$ feature-label subsequence pairs. \n","* With a minibatch size of 2, we only get 3 minibatches.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJf5RwdCdwZE"},"outputs":[],"source":["my_seq = list(range(35))\n","for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n","    print('X: ', X, '\\nY:', Y)"]},{"cell_type":"markdown","metadata":{"id":"ufUjM7LndwZF"},"source":["# Sequential Partitioning\n","\n","* Ensures that the subsequences from two adjacent minibatches during iteration are adjacent in the original sequence.\n","* This strategy preserves the order of split subsequences when iterating over minibatches\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1674657497814,"user":{"displayName":"Ioanna Ntinou","userId":"17583344364006415397"},"user_tz":0},"id":"jBt6X5rAdwZG"},"outputs":[],"source":["def seq_data_iter_sequential(corpus, batch_size, num_steps):  \n","    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n","    # Start with a random offset to partition a sequence\n","    offset = random.randint(0, num_steps)\n","    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n","    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n","    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n","    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n","    num_batches = Xs.shape[1] // num_steps\n","    for i in range(0, num_batches * num_steps, num_steps):\n","        X = Xs[:, i: i + num_steps]\n","        Y = Ys[:, i: i + num_steps]\n","        yield X, Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bCNUgl-dwZG"},"outputs":[],"source":["# Previous example using sequential sampling\n","for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n","    print('X: ', X, '\\nY:', Y)"]},{"cell_type":"markdown","metadata":{"id":"oqrUAsM3dwZH"},"source":["# Loading the data\n","\n","* We use the above functions to define our sequence dataloader"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1674657502378,"user":{"displayName":"Ioanna Ntinou","userId":"17583344364006415397"},"user_tz":0},"id":"fIfeCWTgdwZI"},"outputs":[],"source":["class SeqDataLoader:  \n","    \"\"\"An iterator to load sequence data.\"\"\"\n","    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n","        if use_random_iter:\n","            self.data_iter_fn = mu.seq_data_iter_random\n","        else:\n","            self.data_iter_fn = mu.seq_data_iter_sequential\n","        self.corpus, self.vocab = mu.load_corpus_time_machine(max_tokens)\n","        self.batch_size, self.num_steps = batch_size, num_steps\n","\n","    def __iter__(self):\n","        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i59dERx5dwZI"},"outputs":[],"source":["# similar to load_data_fashion_mnist\n","def load_data_time_machine(batch_size, num_steps,  \n","                           use_random_iter=False, max_tokens=10000):\n","    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n","    data_iter = SeqDataLoader(\n","        batch_size, num_steps, use_random_iter, max_tokens)\n","    return data_iter, data_iter.vocab"]},{"cell_type":"markdown","metadata":{"id":"SnRjtyrXLLj6"},"source":["# Put everything together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIZDEov9LLj7"},"outputs":[],"source":["batch_size, num_steps = 32, 35\n","train_iter, vocab = mu.load_data_time_machine(batch_size, num_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImJUAIFqLLj7"},"outputs":[],"source":["train_iterator = iter(train_iter)\n","batch_1 = next(train_iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxMAtgqdLLj8"},"outputs":[],"source":["sample_1 = batch_1[0][0,:]\n","print(sample_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXz1EJTjLLj9"},"outputs":[],"source":["labels_1 = batch_1[1][0,:]\n","print(labels_1)"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}