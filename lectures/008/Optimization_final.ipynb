{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V2vsEDm3bvFt"},"outputs":[],"source":["# Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYa-rjAU_pQS"},"outputs":[],"source":["%matplotlib inline\n","import my_utils as mu\n","import numpy as np\n","from mpl_toolkits import mplot3d\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"iYQjYKeBbvGA"},"source":["# Stochastic Gradient Descent\n","\n","* In deep learning, the objective function is the average of the losses for each example in the training dataset:\n","$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}).$$\n","   * training dataset with $n$ examples\n","\n","* So, at each iteration, Gradient Descent computes an update using the whole training set: \n","$$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}).$$\n","    * The cost of gradient descent for each iteration will be very high!\n","\n","* Stochastic gradient descent (SGD): at each iteration, randomly sample one example $i\\in\\{1,\\ldots, n\\}$ and compute the gradient $\\nabla f_i(\\mathbf{x})$ to update $\\mathbf{x}$:\n","$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}).$$\n","    * $\\eta$ is the learning rate. \n","* SGD updates are more noisy\n","\n","* Minibatch SGD: at each iteration, randomly sample a batch of examples $\\mathcal{B}$ and compute the gradient $\\sum_{i=1}^{|\\mathcal{B}|}\\nabla f_i(\\mathbf{x})$ to update $\\mathbf{x}$."]},{"cell_type":"markdown","metadata":{"id":"otXvqja5bvGA"},"source":["# Problems with SGD\n","* A single learning rate $\\eta$ is used to update all variables\n","    * for the case of deep learning all optimization parameters\n","* Ideally, we want $\\eta_1$ for $x_1$, $\\eta_2$ for $x_2$, ..., $\\eta_d$ for $x_d$\n","    * Impossible to fix all of these by hand!\n","* Why's this a problem? Consider the following function:\n","$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n","    * $f$ has its minimum at $(0, 0)$. \n","    * This function is *very* flat in the direction of $x_1$. \n","* Let us see what happens when we perform GD with learning rate of $0.4$:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVK5VVKIbvGB","scrolled":true},"outputs":[],"source":["eta = 0.4\n","def f_2d(x1, x2):\n","    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n","def gd_2d(x1, x2, s1, s2):\n","    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n","\n","mu.show_trace_2d(f_2d, mu.train_2d(gd_2d))"]},{"cell_type":"markdown","metadata":{"id":"V-t6u0gYbvGC"},"source":["* The gradient in the $x_2$ direction is *much* higher and changes much more rapidly than in the horizontal $x_1$ direction. \n","* Thus we are stuck between two undesirable choices: \n","    * With a small learning rate we ensure that the solution does not diverge in the $x_2$ direction but we make poor progress in the $x_1$ direction. \n","    * With a large learning rate we progress rapidly in the $x_1$ direction but diverge in $x_2$."]},{"cell_type":"markdown","metadata":{"id":"90Ki1CGAbvGC"},"source":["* Let's increase learning rate from $0.4$ to $0.6$. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIKcuTNhbvGC"},"outputs":[],"source":["eta = 0.6\n","mu.show_trace_2d(f_2d, mu.train_2d(gd_2d))"]},{"cell_type":"markdown","metadata":{"id":"tn6tgmtVbvGD"},"source":["* Convergence in the $x_1$ direction improves but the overall solution quality is much worse."]},{"cell_type":"markdown","metadata":{"id":"F7qJbFw_bvGD"},"source":["# The Momentum Method\n","\n","* The momentum method allows us to solve the gradient descent problem described above. \n","\n","* Looking at the optimization trace above we might think that **averaging gradients over the past** would work well. \n","    * In the $x_1$ direction this will aggregate well-aligned gradients, thus increasing the distance we cover with every step. \n","    * In the $x_2$ direction where gradients oscillate, an aggregate gradient will reduce step size due to oscillations that cancel each other out.\n","    \n","* Instead of the gradient $\\mathbf{g}_t = \\sum_{i=1}^{|\\mathcal{B}|}\\nabla f_i(\\mathbf{x})$, use $\\mathbf{v}_t$  to update:\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{v}_t &\\leftarrow \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t}, \\\\\n","\\mathbf{x}_t &\\leftarrow \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n","\\end{aligned}\n","$$\n","\n","* For $\\beta = 0$ we recover regular GD descent. \n","* Let's consider optimizing the same function as above using GD with momemtum.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tv_Xyw5PbvGE","scrolled":true},"outputs":[],"source":["def momentum_2d(x1, x2, v1, v2):\n","    v1 = beta * v1 + 0.2 * x1\n","    v2 = beta * v2 + 4 * x2\n","    return x1 - eta * v1, x2 - eta * v2, v1, v2\n","\n","eta, beta = 0.6, 0.5\n","mu.show_trace_2d(f_2d, mu.train_2d(momentum_2d))"]},{"cell_type":"markdown","metadata":{"id":"YIyDJ-SLbvGE"},"source":["* Even with the same learning rate that we used before, momentum still converges well!"]},{"cell_type":"markdown","metadata":{"id":"B1aw82k7bvGE"},"source":["* Let us see what happens when we decrease the momentum parameter. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NljGtKxtbvGF"},"outputs":[],"source":["eta, beta = 0.6, 0.25\n","mu.show_trace_2d(f_2d, mu.train_2d(momentum_2d))"]},{"cell_type":"markdown","metadata":{"id":"J1E9ui09bvGF"},"source":["* Halving it to $\\beta = 0.25$ leads to a trajectory that barely converges at all. \n","    * Nonetheless, it is a lot better than without momentum (when the solution diverges)."]},{"cell_type":"markdown","metadata":{"id":"dVhm_ezdbvGF"},"source":["# RMS-Prop\n","\n","* This is an adaptive method which produces essentially a different effective learning rate for each optimization variable $x_1, x_2, \\dots, x_d$.\n","* Main idea: if the anisitropic shape of the optimization function could become isotropic (like a radius), then we could use a single learning rate for all optimization variables.\n","* To achieve this RMS-PROP normalizes (divides) each derivative by its magnitute. \n","    * it actually keeps track of an average magnitude over past samples.\n","\n","$$\\begin{aligned}\n","    \\mathbf{g}_t &= \\sum_{i=1}^{|\\mathcal{B}|}\\nabla f_i(\\mathbf{x}), \\\\\n","    \\mathbf{s}_t & \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\mathbf{g}_t^2, \\\\\n","    \\mathbf{x}_t & \\leftarrow \\mathbf{x}_{t-1} - \\frac{\\eta \\mathbf{g}_t}{\\sqrt{\\mathbf{s}_t + \\epsilon}}.\n","\\end{aligned}$$\n","\n","* The constant $\\epsilon > 0$ is typically set to $10^{-6}$ to ensure that we do not suffer from division by zero."]},{"cell_type":"markdown","metadata":{"id":"C6EssEsCbvGF"},"source":["# ADAM\n","\n","* Yes it's the guy from Dark (no he's not!)\n","* Adam combines momentum with RMS-Prop\n","* If you check RMS-Prop update equation above the gradient $\\mathbf{g}_t$ is used.\n","* Instead we can use the same quantity $\\mathbf{v}_t$ we used in momentum. Overall we have:\n","\n","$$\\begin{aligned}\n","    \\mathbf{v}_t & \\leftarrow \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\mathbf{g}_t, \\\\\n","    \\mathbf{s}_t & \\leftarrow \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2.\n","\\end{aligned}$$\n","\n","* Here $\\beta_1$ and $\\beta_2$ are nonnegative weighting parameters. \n","    * Common choices for them are $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. \n","\n","* If we initialize $\\mathbf{v}_0 = \\mathbf{s}_0 = 0$ we have a significant amount of bias initially towards smaller values. To address this we use the fact that $\\sum_{i=0}^t \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$ to re-normalize terms. Hence, the normalized state variables are given by: \n","\n","$$\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t} \\text{ and } \\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t}.$$\n","\n","* Now we have all the pieces in place to compute the updates: \n","\n","$$\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t-1} - \\frac{\\eta \\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}.$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jZn7BouvbvGG"},"source":["# Summary\n","\n","* In high dimensions adjusting the learning rate is complicated.\n","* Momentum replaces gradients with a leaky average over past gradients. This accelerates convergence significantly.\n","* RMS-Prop produces essentially a different effective learning rate for each optimization variable by dividing each elenemnt of the gradient with its magnitude.\n","* Adam combines RMS-Prop with momentum.\n"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}