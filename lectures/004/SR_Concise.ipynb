{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PHj-TUOEcErV"},"outputs":[],"source":["#Uncomment if you're using colab\n","#Setting up google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xieUj0_Wamk3"},"outputs":[],"source":["import my_utils as mu\n","import torch\n","from torch import nn\n","from IPython import display"]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"8WU8N5biamlF"},"source":["# Concise Implementation of Softmax Regression\n","\n","* Goal: use high-level APIs of PyTorch for implementing Softmax Regression for classification. "]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":4,"tab":["pytorch"],"id":"ikRExUfEamlK"},"outputs":[],"source":["batch_size = 256\n","train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)\n","# type(train_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnjzW0FnamlN"},"outputs":[],"source":["X, y = next(iter(train_iter)) # first batch\n","print(X.size())\n","print(y)"]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"SWzAl_2xamlS"},"source":["# Defining the Model and Initialization\n","\n","* Each example is represented by a fixed-length vector: we flatten each $28 \\times 28$ image, treating it as vector of length 784.\n","\n","* Because our dataset has 10 classes, our network will have an output dimension of 10.\n","* So, our weights `W` will be a $784 \\times 10$ matrix and the biases `b` will constitute a $10 \\times 1$ row vector.\n","* We initialize `W` using a Gaussian distribution and `b` with 0.\n","* Softmax regression can be implemented as a Fully-Connected (i.e Linear) layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":7,"tab":["pytorch"],"id":"haE84ks7amlV"},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self, num_inputs, num_outputs):\n","        super(Net, self).__init__()\n","        self.num_inputs = num_inputs\n","        self.num_outputs = num_outputs\n","        self.Linear1 = nn.Linear(num_inputs, num_outputs)\n","        torch.nn.init.normal_(self.Linear1.weight, std=0.01) #init the weights\n","        torch.nn.init.zeros_(self.Linear1.bias) #init the bias\n","        \n","    def forward(self, x):\n","        x = x.view(-1, self.num_inputs)\n","        out = self.Linear1(x)\n","        return out\n","\n","num_inputs, num_outputs = 784, 10\n","net = Net(num_inputs, num_outputs)\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"5yeLoHFgamlY"},"source":["# Alternative Initialization\n","* This is useful if you have multiple layers of the same type and you want them to be initialized in the same way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCvXctvFamlb"},"outputs":[],"source":["def init_weights(m):\n","    if isinstance(m, nn.Linear): # by checking type we can init different layers in different ways\n","        torch.nn.init.normal_(m.weight, std=0.01)\n","        torch.nn.init.zeros_(m.bias)\n","\n","net.apply(init_weights);\n","# print(net)"]},{"cell_type":"markdown","metadata":{"id":"hXkoneIQamle"},"source":["# Loss Function\n","\n","* Use PyTorch's implementation of Softmax-Cross Entropy loss to avoid numerical instabilities.\n","    * The input to loss function are the logits logits $\\mathbf{o}$ (and not softmax outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":11,"tab":["pytorch"],"id":"-QdfLPIqamlg"},"outputs":[],"source":["loss = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"origin_pos":13,"id":"8oMmvJ_3amli"},"source":["# Optimization Algorithm\n","\n","\n","* Minibatch SGD with a learning rate of 0.1 as the optimization algorithm.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":15,"tab":["pytorch"],"id":"5WeTKzL2amlk"},"outputs":[],"source":["optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"]},{"cell_type":"markdown","metadata":{"id":"xPIzec6Yamll"},"source":["# Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnRvQeCbamln","executionInfo":{"status":"ok","timestamp":1674646799294,"user_tz":0,"elapsed":251,"user":{"displayName":"Ioanna Ntinou","userId":"17583344364006415397"}},"outputId":"69bdaad7-2434-4b68-aafb-0ed811aacdd2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":12}],"source":["def accuracy(y_hat, y):  #y_hat is a matrix; 2nd dimension stores prediction scores for each class.\n","    \"\"\"Compute the number of correct predictions.\"\"\"\n","    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n","        y_hat = y_hat.argmax(axis=1) # Predicted class is the index of max score         \n","    cmp = (y_hat.type(y.dtype) == y)  # because`==` is sensitive to data types\n","    return float(torch.sum(cmp)) # Taking the sum yields the number of correct predictions.\n","\n","# Example: only 1 sample is correctly classified.\n","y = torch.tensor([0, 2])\n","y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n","accuracy(y_hat, y) / len(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t52Xdrfjamlp"},"outputs":[],"source":["class Accumulator:  \n","    \"\"\"For accumulating sums over `n` variables.\"\"\"\n","    def __init__(self, n):\n","        self.data = [0.0] * n # [0, 0, ..., 0]\n","    def add(self, *args):\n","        self.data = [a + float(b) for a, b in zip(self.data, args)]\n","    def reset(self):\n","        self.data = [0.0] * len(self.data)\n","    def __getitem__(self, idx):\n","        return self.data[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlmgKHjaamlr"},"outputs":[],"source":["def evaluate_accuracy(net, data_iter): \n","    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n","    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n","    for _, (X, y) in enumerate(data_iter):\n","        metric.add(accuracy(net(X), y), y.numel())\n","    return metric[0] / metric[1]"]},{"cell_type":"markdown","metadata":{"id":"eow0yz-_amls"},"source":["* The accuracy of the model prior to training should be close to random guessing, i.e., 0.1 for 10 classes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WemEpFTbamlu"},"outputs":[],"source":["evaluate_accuracy(net, test_iter)"]},{"cell_type":"markdown","metadata":{"origin_pos":17,"id":"d-Y8g9qRamlv"},"source":["# Training\n","\n","* The training loop for softmax regression looks strikingly familiar with that of linear regression. \n","* Here we refactor the implementation to make it reusable.\n","    * First, we define a function to train for one epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HB63NEzramlw"},"outputs":[],"source":["def train_epoch_ch3(net, train_iter, loss, optimizer):  \n","    \"\"\"The training function for one epoch.\"\"\"\n","    # Set the model to training mode\n","    if isinstance(net, torch.nn.Module):\n","        net.train()\n","    # Sum of training loss, sum of training accuracy, no. of examples\n","    metric = Accumulator(3)\n","    for X, y in train_iter:\n","        # Compute gradients and update parameters\n","        y_hat = net(X)\n","        l = loss(y_hat, y)\n","        optimizer.zero_grad()\n","        l.backward()\n","        optimizer.step()\n","        metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel())\n","    # Return training loss and training accuracy\n","    return metric[0] / metric[2], metric[1] / metric[2]"]},{"cell_type":"markdown","metadata":{"id":"YJeBipHjamly"},"source":["# Training\n","\n","* The following class will be used to plot training and validation accuracy as well as loss evolution over training loop. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t14hhWpTamlz"},"outputs":[],"source":["class Animator:  \n","    \"\"\"For plotting data in animation.\"\"\"\n","    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n","                 ylim=None, xscale='linear', yscale='linear',\n","                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n","                 figsize=(3.5, 2.5)):\n","        # Incrementally plot multiple lines\n","        if legend is None:\n","            legend = []\n","        mu.use_svg_display()\n","        self.fig, self.axes = mu.plt.subplots(nrows, ncols, figsize=figsize)\n","        if nrows * ncols == 1:\n","            self.axes = [self.axes, ]\n","        # Use a lambda function to capture arguments\n","        self.config_axes = lambda: mu.set_axes(\n","            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n","        self.X, self.Y, self.fmts = None, None, fmts\n","\n","    def add(self, x, y):\n","        # Add multiple data points into the figure\n","        if not hasattr(y, \"__len__\"):\n","            y = [y]\n","        n = len(y)\n","        if not hasattr(x, \"__len__\"):\n","            x = [x] * n\n","        if not self.X:\n","            self.X = [[] for _ in range(n)]\n","        if not self.Y:\n","            self.Y = [[] for _ in range(n)]\n","        for i, (a, b) in enumerate(zip(x, y)):\n","            if a is not None and b is not None:\n","                self.X[i].append(a)\n","                self.Y[i].append(b)\n","        self.axes[0].cla()\n","        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n","            self.axes[0].plot(x, y, fmt)\n","        self.config_axes()\n","        display.display(self.fig)\n","        display.clear_output(wait=True)"]},{"cell_type":"markdown","metadata":{"id":"WchkKt-9aml1"},"source":["# Training\n","\n","* The following function trains the model (`net`) on a training set (`train_iter`) for `num_epochs`.\n","* At the end of each epoch, the model is evaluated on a testing set (`test_iter`).\n","* `Animator` for visualizing the training progress."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3lM9r-taml2"},"outputs":[],"source":["def train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer):  \n","    \"\"\"Train a model.\"\"\"\n","    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","    for epoch in range(num_epochs):\n","        train_metrics = train_epoch_ch3(net, train_iter, loss, optimizer)\n","        test_acc = evaluate_accuracy(net, test_iter)\n","        animator.add(epoch + 1, train_metrics + (test_acc,))\n","    train_loss, train_acc = train_metrics\n","    assert train_loss < 0.5, train_loss\n","    assert train_acc <= 1 and train_acc > 0.7, train_acc\n","    assert test_acc <= 1 and test_acc > 0.7, test_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":18,"scrolled":false,"tab":["pytorch"],"id":"9xUyKHM_aml3"},"outputs":[],"source":["num_epochs = 30\n","train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"ixCZxBdUaml4"},"source":["# Summary\n","\n","* Using PyTorch's high-level APIs, we can implement softmax regression much more concisely.\n","* From a computational perspective, implementing softmax regression has intricacies. \n","* Note that in many cases, PyTorch takes additional precautions beyond these most well-known tricks to ensure numerical stability"]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}