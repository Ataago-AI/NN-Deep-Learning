{"cells":[{"cell_type":"markdown","metadata":{"id":"Od_NYpeKGnAD"},"source":["# Softmax Regression\n","\n","* Regression is the hammer we reach for when we want to answer *how much?* or *how many?* questions.\n","    * If you want to predict the number of pounds (price) at which a house will be sold, or the number of days that a patient will remain hospitalized before being discharged, then you are probably looking for a regression model.\n","\n","* In many cases, we are interested in *classification*: asking not \"how much\" but \"which one\":\n","    * Does this email belong in the spam folder or the inbox?\n","    * Does this image depict a donkey, a dog, a cat, or a rooster?"]},{"cell_type":"markdown","metadata":{"id":"X4QG148gGnAL"},"source":["# Classification Problem\n","\n","* **Simple Example:** a simple image classification problem.\n","  * Each input is $2\\times2$ grayscale image.\n","    * Each pixel value is a single scalar, giving us four features $x_1, x_2, x_3, x_4$.\n","  * We also assume that each image belongs to one of the \"cat\", \"chicken\", and \"dog\" classes"]},{"cell_type":"markdown","metadata":{"id":"Twk76jOrGnAM"},"source":["# Classification Problem\n","## Labels as one-hot encoding\n","\n","* We have to choose how to represent the labels.\n","* We have the following obvious choice: choose $y \\in \\{1, 2, 3\\}$, for $\\{\\text{dog}, \\text{cat}, \\text{chicken}\\}$ respectively.\n","    * This introduces ordering which is not desired.\n","\n","* Better choice: the **one-hot encoding**.\n","    * A vector with as many components as the number of classes.\n","    * The component corresponding to a particular instance's class is set to 1 and all other components are set to 0.\n","\n","* In our case, a label $y$ would be a three-dimensional vector, with $(1, 0, 0)$ corresponding to \"cat\", $(0, 1, 0)$ to \"chicken\", and $(0, 0, 1)$ to \"dog\":\n","\n","$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$"]},{"cell_type":"markdown","metadata":{"id":"2z1FLqf6GnAM"},"source":["# Network Architecture\n","\n","* We need a model with multiple outputs, one per element of the one-hot encoding (i.e. one per class), i.e. 3 outputs in total. \n","* For a training example, we will pass the features through the model and get as output 3 numbers.\n","    * These will not be exactly zeros and ones.\n","* The location of the maximum indicates the correct class.\n","\n","* Since we have 4 features we need, *for each output*, 4 weights and 1 bias which will be used to linearly combine the input features to give us an output. \n","* We compute 3 outputs or *logits*, $o_1, o_2$, and $o_3$, for each input:\n","\n","$$\n","\\begin{aligned}\n","o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n","o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n","o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n","\\end{aligned}\n","$$\n","\n","* To express the model more compactly, we can use linear algebra: $\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$,\n","    * All features of a given example are gathered in vector $\\mathbf{x}$\n","    * All weights are gathered into a $3 \\times 4$ matrix $\\mathbf{W}$\n","    * All biases are gathered in vector $\\mathbf{b}$\n","    * All outputs are gathered in vector $\\mathbf{o}$"]},{"cell_type":"markdown","metadata":{"id":"1M52y4f1GnAN"},"source":["# Network Architecture\n","\n","* We can depict this calculation with a neural network diagram:\n","\n","<!-- ![Softmax regression is a single-layer neural network.](img/softmaxreg.svg)  -->\n","\n","![Softmax regression is a single-layer neural network.](https://drive.google.com/uc?export=view&id=12RSRmm8SkL-JCSvDGnV2L2VGzgOCpWqy) \n","\n","\n","* Just as in linear regression, softmax regression is also a single-layer neural network.\n","  * Since the calculation of each output, $o_1, o_2$, and $o_3$, depends on all inputs, $x_1$, $x_2$, $x_3$, and $x_4$, softmax regression can also be implemented with a fully-connected layer."]},{"cell_type":"markdown","metadata":{"id":"Dw_E2qVLGnAN"},"source":["\n","# Softmax Operation\n","* For training the model, our approach will be to interpret the outputs of our model as probabilities.\n","    * We will then optimize the model parameters to produce probabilities that maximize the likelihood of the observed data.\n","\n","* One option to consider is to interpret the logits $o$ as probabilities. However: \n","    * Nothing constrains these numbers to sum to 1.\n","    * Logits $o$ can take negative values.\n","        * Both these violate the basic axioms of probability.\n","\n","* The **Softmax function** can be used to transform our logits such that they become nonnegative and sum to 1.\n","    * The model remains differentiable."]},{"cell_type":"markdown","metadata":{"id":"aF-a19v1GnAO"},"source":["# Softmax Operation\n","\n","* First exponentiate each logit (ensuring non-negativity) and then divide by their sum (ensuring they sum to 1):\n","$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$\n","* It is easy to see $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$ with $0 \\leq \\hat{y}_j \\leq 1$ for all $j$.\n","    * Thus, $\\hat{\\mathbf{y}}$ is a proper probability distribution whose element values can be interpreted accordingly.\n","    * Softmax operation does not change the ordering among the logits $\\mathbf{o}$,\n","* During prediction we can still pick out the most likely class by\n","\n","$$\n","\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j.\n","$$\n","\n","* For example, if $\\hat{y}_1$, $\\hat{y}_2$, and $\\hat{y}_3$ are 0.1, 0.8, and 0.1, then we predict category 2, which (in our example) represents \"chicken\".\n","\n","* Although softmax is a nonlinear function, the outputs of softmax regression are still *determined* by a linear transformation of input features; thus, softmax regression is a linear model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WcmCjpXBGnAP"},"source":["# Vectorization for Minibatches\n","\n","* We typically carry out training in minibatches of data.\n","* Assume minibatch $\\mathbf{X}$ of examples with feature dimensionality $d$ and batch size $n$. Moreover, assume $q$ classes in the output. Then:\n","    * $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$, $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$,\n","and $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n","\n","$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n","\n","* Since each row in $\\mathbf{X}$ represents a data point, the softmax operation can be computed *rowwise*:\n","    * for each row of $\\mathbf{O}$, exponentiate all entries and then normalize them by the sum."]},{"cell_type":"markdown","metadata":{"id":"Lo_9fbqlGnAP"},"source":["# Loss Function\n","\n","* For training the model, we need a loss function to measure the quality of our predicted probabilities.\n","* We will rely on Maximum Likelihood estimation\n","    * the very same concept encountered when providing a probabilistic justification for the mean squared error objective in linear regression\n"]},{"cell_type":"markdown","metadata":{"id":"uzHpqbH0GnAR"},"source":["\n","# Loss Function\n","\n","* The softmax function gives us a vector $\\hat{\\mathbf{y}}$, which can be interpreted as the estimated (conditional) probabilities of each class given an input $\\mathbf{x}$. \n","    * E.g., $\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$.\n","\n","* Suppose that the entire dataset $\\{\\mathbf{X}, \\mathbf{Y}\\}$ has $n$ examples, \n","* $i-$th example has feature vector $\\mathbf{x}^{(i)}$ and a one-hot label vector $\\mathbf{y}^{(i)}$.  \n","    * If the correct class for the $i-$th example is the $k-$th class, then the $k-$th element of $\\mathbf{y}^{(i)}$, denoted as $y^{(i)}_k$ will be 1 and all other elements will be 0. \n","    * According to Maximum Likelihood we want to maximize the probability that the model assigns the correct class to the $i-$th example: \n","    \n","    $$P(y=y^{(i)}_k \\mid\\mathbf{x}^{(i)})=\\hat{y}_k$$"]},{"cell_type":"markdown","metadata":{"id":"Mbb4U4uTGnAR"},"source":["# Loss Function\n","\n","* This can be also written as: \n","    \n","$$\n","\\begin{aligned}\n","    P(y=y^{(i)}_k \\mid\\mathbf{x}^{(i)}) &= P(y=y^{(i)}_k \\mid\\mathbf{x}^{(i)})^{y_{k}^{(i)}}\\\\\n","                                                 &= \\prod_{j=1}^q P(y=y^{(i)}_j \\mid\\mathbf{x}^{(i)})^{y_{j}^{(i)}}\\\\\n","                                                 &= \\prod_{j=1}^q \\hat{y}_j^{y_{j}^{(i)}}\n","\\end{aligned}\n","$$\n","    \n","* This is equivalent to minimizing the negative log-likelihood:\n","$$ l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}) = - \\sum_{j=1}^q y_j^{(i)} \\log \\hat{y}_j^{(i)} $$"]},{"cell_type":"markdown","metadata":{"id":"OtqDP1a_GnAS"},"source":["# Cross-Entropy Loss\n","\n","* The total loss is calcluated over all examples:\n","\n","$$CE = \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}) $$\n","\n","* This is called **Cross-Entopy Loss**.\n","* Since $\\mathbf{y}$ is a one-hot vector of length $q$, the sum over all its coordinates $j$ vanishes for all but one term.\n","* Since all $\\hat{y}_j$ are predicted probabilities, their logarithm is never larger than $0$.\n","* Consequently, the loss function cannot be minimized any further if we correctly predict the actual label with *certainty*, i.e., if the predicted probability $P(\\mathbf{y} \\mid \\mathbf{x}) = 1$ for the actual label $\\mathbf{y}$.\n","* Note that this may also not be possible when the input features are not sufficiently informative to classify every example perfectly."]},{"cell_type":"markdown","metadata":{"id":"aw5adYfRGnAS"},"source":["# Cross-Entropy Loss Derivatives\n","\n","* Plugging softmax output into the definition of the loss:\n","\n","$$\n","\\begin{aligned}\n","l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n","&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n","&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n","\\end{aligned}\n","$$\n","\n","* Consider now the derivative with respect to any logit $o_j$:\n","\n","$$\n","\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n","$$\n","\n","* The derivative is the difference between the probability produced by our model,\n","and the elements in the one-hot label vector.\n","* Very similar to what we saw in regression, where the gradient was the difference between the observation $y$ and estimate $\\hat{y}$.\n","* This fact makes computing gradients easy in practice."]},{"cell_type":"markdown","metadata":{"id":"yA5kAdU4GnAT"},"source":["\n","# Model Prediction and Evaluation\n","\n","* After training the model, given any example features, we can predict the probability of each output class.\n","* We use the class with the highest predicted probability as the output class.\n","* The prediction is correct if it is consistent with the actual class (label)."]},{"cell_type":"markdown","metadata":{"id":"_v-oCVrpGnAU"},"source":["# Summary\n","\n","* The softmax operation takes a vector and maps it into probabilities.\n","* Softmax regression applies to classification problems. \n","    * It uses the probability distribution of the output class in the softmax operation."]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}